{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TMVA Classification Using Deep Neural Networks</center>\n",
    "\n",
    "In this notebook we still classify di-Higgs new data with Deep Neural Networks meethod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA, TTree\n",
    "import pandas as pd\n",
    "\n",
    "ROOT.TMVA.Tools.Instance()\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize()\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from array import array\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by region.\n",
    "\n",
    "This function will let you filter your dataset by region. It's known that SR_1tag is very signal poor, while SR_2tag has a lot a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_region(file, region, signal):\n",
    "    oldfile = ROOT.TFile(file)\n",
    "    oldtree = oldfile.Nominal\n",
    "    signal_file = ROOT.TFile(region+\"_\"+signal+\"_s.root\",\"recreate\")\n",
    "    signal_tree = oldtree.CloneTree(0)\n",
    "    backg_file = ROOT.TFile(region+\"_\"+signal+\"_b.root\",\"recreate\")\n",
    "    backg_tree = oldtree.CloneTree(0)\n",
    "    data_file = ROOT.TFile(region+\"_\"+signal+\"_d.root\",\"recreate\")\n",
    "    data_tree = oldtree.CloneTree(0)\n",
    "    for entry in oldtree:\n",
    "        if (entry.m_region == region):\n",
    "            if (entry.sample == \"data\"):\n",
    "                data_tree.Fill()\n",
    "            elif (entry.sample == \"Xtohh1000\"): #signal\n",
    "                signal_tree.Fill()\n",
    "            else:\n",
    "                backg_tree.Fill()\n",
    "    signal_tree.AutoSave()   \n",
    "    backg_tree.AutoSave()\n",
    "    data_tree.AutoSave()\n",
    "    return signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file\n",
    "\n",
    "#Use as\n",
    "#tree, file = filter_region(\"data.root\", \"SR_1tag\", \"small.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory and Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.root has unlabeled data points (called data) and fakes points. For the background training we'll use only the fakes points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file = filter_region(\"all_1000.root\", \"SR_1tag\", \"Xtohh1000\")\n",
    "\n",
    "outputFile = ROOT.TFile.Open(\"Higgs_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "# Factory\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                      \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "#signal_tree.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input data abd variables \n",
    "\n",
    "We add first the signal and background trees in the data loader and then we\n",
    "define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "We have two kinds of signals and for the training we have to use only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree    ( signal_tree )\n",
    "loader.AddBackgroundTree( backg_tree )\n",
    "loader.SetWeightExpression(\"EventWeight\")\n",
    "\n",
    "not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "## Define input variables \n",
    "for branch in backg_tree.GetListOfBranches():\n",
    "    if branch.GetName() in not_cons:\n",
    "        continue\n",
    "    loader.AddVariable(branch.GetName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Setup the DataLoader by splitting events in training and test samples. \n",
    "Here we use a random split and a fixed number of training and test events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "loader.PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=70%:nTrain_Background=70%:SplitMode=Random:\"\n",
    "                                   \"NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='sigmoid', input_dim=10))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy',])\n",
    "\n",
    "# Store model to file\n",
    "model.save('model_dense.h5')\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ROOT.TMVA::MethodPyKeras object (\"Keras_Dense\") at 0x923bcf0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, 'Keras_Dense',\n",
    "        'H:!V:VarTransform=G:FilenameModel=model_dense.h5:'+\\\n",
    "        'NumEpochs=10:BatchSize=16:TriesEarlyStopping=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7272 - categorical_accuracy: 0.4911 - val_loss: 0.6359 - val_categorical_accuracy: 0.4286\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63592, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 0s 177us/step - loss: 0.6905 - categorical_accuracy: 0.5625 - val_loss: 0.6317 - val_categorical_accuracy: 0.5714\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63592 to 0.63165, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 0s 172us/step - loss: 0.6732 - categorical_accuracy: 0.6071 - val_loss: 0.6349 - val_categorical_accuracy: 0.4286\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63165\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 0s 189us/step - loss: 0.6678 - categorical_accuracy: 0.5000 - val_loss: 0.6381 - val_categorical_accuracy: 0.5357\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63165\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 0s 170us/step - loss: 0.6412 - categorical_accuracy: 0.6071 - val_loss: 0.6344 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63165\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 0s 181us/step - loss: 0.6218 - categorical_accuracy: 0.6339 - val_loss: 0.6361 - val_categorical_accuracy: 0.4286\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63165\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 0s 186us/step - loss: 0.6028 - categorical_accuracy: 0.6161 - val_loss: 0.6427 - val_categorical_accuracy: 0.4286\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63165\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 0s 189us/step - loss: 0.5808 - categorical_accuracy: 0.5357 - val_loss: 0.6426 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.63165\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 0s 205us/step - loss: 0.5569 - categorical_accuracy: 0.5714 - val_loss: 0.6338 - val_categorical_accuracy: 0.4286\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.63165\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 0s 202us/step - loss: 0.5155 - categorical_accuracy: 0.6429 - val_loss: 0.6362 - val_categorical_accuracy: 0.4286\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.63165\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.716\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.030 (0.210)       0.306 (0.616)      0.640 (0.735)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods()\n",
    "\n",
    "# Here we test all methods using the test data set\n",
    "factory.TestAllMethods()\n",
    "\n",
    "# Here we evaluate all methods and compare their performances, computing efficiencies, \n",
    "# ROC curves etc.. using both training and tetsing data sets. Several histograms are \n",
    "# produced which can be examined with the TMVAGui or directly using the output file\n",
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(file):\n",
    "    params = []\n",
    "    first = 1\n",
    "    with open(file, 'r') as fp:\n",
    "        line = fp.readline().rstrip()\n",
    "        while line:\n",
    "            if (file.split('.')[1] == 'csv' and first):\n",
    "                first = 0\n",
    "                line = fp.readline().rstrip()\n",
    "                continue\n",
    "            params.append(line)\n",
    "            line = fp.readline().rstrip()       \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(params, training, model_input, comp_params, model_name, config):\n",
    "    \n",
    "    output_file = config+\"_DNN_Classification.root\"\n",
    "    signal_file = config+\"_s.root\"\n",
    "    backg_file = config+\"_b.root\"\n",
    "    \n",
    "    signal_input = ROOT.TFile(signal_file)\n",
    "    signal_tree = signal_input.Nominal\n",
    "    \n",
    "    backg_input = ROOT.TFile(backg_file)\n",
    "    backg_tree = backg_input.Nominal\n",
    "    \n",
    "    outputFile = ROOT.TFile.Open(output_file, \"RECREATE\")\n",
    "\n",
    "    # Factory\n",
    "    factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification_\"+config, outputFile,\n",
    "                          \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "    loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "    ### global event weights per tree (see below for setting event-wise weights)\n",
    "    #signalWeight     = 1.0\n",
    "    #backgroundWeight = 1.0\n",
    "\n",
    "    ### You can add an arbitrary number of signal or background trees\n",
    "    loader.AddSignalTree    ( signal_tree )\n",
    "    loader.AddBackgroundTree( backg_tree )\n",
    "    loader.SetWeightExpression(\"EventWeight\")\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "    ## Define input variables \n",
    "    for branch in backg_tree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        loader.AddVariable(branch.GetName())\n",
    "        \n",
    "    mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "    mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "    loader.PrepareTrainingAndTestTree(mycuts, mycutb, training)\n",
    "    \n",
    "    # Model structure\n",
    "    \n",
    "    comp_params = comp_params.rstrip()\n",
    "    comp_params = comp_params.split(',')\n",
    "    loss = comp_params[0]\n",
    "    \n",
    "    comp_params.remove(loss)\n",
    "    metrics = comp_params\n",
    "    \n",
    "    model = Sequential()\n",
    "    model_input = model_input.rstrip()\n",
    "    model_input = model_input.split(',')\n",
    "    \n",
    "    hidden_l = int(model_input[0])\n",
    "    neurons = int(model_input[1])\n",
    "    neurons_LF = int(model_input[2])\n",
    "    k_init = model_input[3]\n",
    "    activation_IL = model_input[4]\n",
    "    activation_HL = model_input[5]\n",
    "    activation_FL = model_input[6]\n",
    "    \n",
    "    print(type(neurons))\n",
    "    \n",
    "    model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_IL, input_dim=10))\n",
    "    for h in range(hidden_l):\n",
    "        model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_HL))\n",
    "        \n",
    "    model.add(Dense(neurons_LF, kernel_initializer=k_init, activation=activation_FL))\n",
    "    \n",
    "    # Set loss and optimizer\n",
    "    model.compile(loss=loss, optimizer=Adam(), metrics=metrics)\n",
    "    # Store model to file\n",
    "    model.save(model_name)\n",
    "    # Print summary of model\n",
    "    model.summary()\n",
    "    \n",
    "    ## DNN method\n",
    "    factory.BookMethod(loader,ROOT.TMVA.Types.kPyKeras, \"Keras_Dense\", params)\n",
    "        \n",
    "    factory.TrainAllMethods()\n",
    "    \n",
    "    factory.TestAllMethods()\n",
    "    \n",
    "    factory.EvaluateAllMethods()\n",
    "    \n",
    "    c1 = factory.GetROCCurve(loader)\n",
    "    #c1.Draw()\n",
    "    \n",
    "    integ = factory.GetROCIntegral(loader, \"Keras_Dense\")\n",
    "    \n",
    "    print(\"ROC integral:\", integ)\n",
    "    \n",
    "    outputFile.Close()\n",
    "    signal_input.Close()\n",
    "    backg_input.Close()\n",
    "    \n",
    "    return integ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_combs_params(file_params, file_training, file_model, comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics):\n",
    "    comb_params = list(itertools.product(arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics))\n",
    "    with open(file_params, 'w') as params, open(file_training, 'w') as training, open(file_model, 'w') as model, open(comp_params, 'w') as comp_p:\n",
    "        model.write(\"number_HL,neurons,neurons_LF,k_init,activation_IL,activation_HL,activation_FL\\n\")\n",
    "        for cp in comb_params:\n",
    "            string1 = \"H:!V:VarTransform=N_AllClasses:FilenameModel=\"+model_name+\":NumEpochs=\"+str(cp[0])+\":BatchSize=\"+str(cp[1])+\":TriesEarlyStopping=10\\n\"\n",
    "            params.write(string1)\n",
    "            string2 = \"nTrain_Signal=\"+str(cp[2])+\"%:nTrain_Background=\"+str(cp[3])+\"%:SplitMode=Random:NormMode=NumEvents:!V\\n\"\n",
    "            training.write(string2)\n",
    "            string3 = str(cp[4])+','+str(cp[5])+','+str(cp[6])+','+str(cp[7])+','+str(cp[8])+','+str(cp[9])+','+str(cp[10])+'\\n'\n",
    "            model.write(string3)\n",
    "            string4 = str(cp[11])+','+str(cp[12])+'\\n'\n",
    "            comp_p.write(string4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_params=\"dnn_params2.txt\"\n",
    "file_training=\"dnn_training2.txt\"\n",
    "file_model=\"dnn_model2.csv\"\n",
    "file_comp_params='comp_params.txt'\n",
    "model_name=\"model_dense.h5\"\n",
    "arr_NumEpochs=[100, 200]\n",
    "arr_BatchSize=[100, 200]\n",
    "arr_nTrain_Signal=[80]\n",
    "arr_nTrain_Background=[80]\n",
    "arr_number_HL=[3]\n",
    "arr_neurons=[64]\n",
    "arr_neurons_LF=[2]\n",
    "arr_k_init=['glorot_normal']\n",
    "arr_activation_IL=['sigmoid']\n",
    "arr_activation_HL=['relu']\n",
    "arr_activation_FL=['softmax']\n",
    "arr_loss=['categorical_crossentropy']\n",
    "arr_metrics=['categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_combs_params(file_params, file_training, file_model, file_comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_opt(config, params, training, model, comp_params, model_name):\n",
    "    max_roc = 0\n",
    "    best_params = \"\"\n",
    "    best_train = \"\"\n",
    "    best_model = \"\"\n",
    "    print(config)\n",
    "    print(\"===============\")\n",
    "    for i in range(len(params)):\n",
    "        roc = DNN(params[i], training[i], model[i], comp_params[i], model_name, config)\n",
    "        if roc > max_roc:\n",
    "            max_roc = roc\n",
    "            best_params = params[i]\n",
    "            best_train = training[i]\n",
    "            best_model = model[i]\n",
    "    best_model = best_model.split(',')\n",
    "    best_model_str = \"numero_HL=\"+str(best_model[0])+\", neurons=\"+str(best_model[1])+\", neurons_LF=\"+str(best_model[2])+\", k_init=\"+str(best_model[3])+\", activation_IL=\"+str(best_model[4])+\", activation_HL=\"+str(best_model[5])+\", activation_FL=\"+str(best_model[6])\n",
    "    print(\"best parameters:\", best_params)\n",
    "    print(\"best training:\", best_train)\n",
    "    print(\"best model:\", best_model_str)\n",
    "    print(\"ROC integral:\", max_roc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"dnn_params2.txt\"\n",
    "params = get_params(params_path)\n",
    "training_path = \"dnn_training2.txt\"\n",
    "training = get_params(training_path)\n",
    "configs = [\"PreSel_0tag_Xtohh1000\", \"PreSel_1tag_Xtohh1000\", \"PreSel_2tag_Xtohh1000\", \n",
    "           \"QCDCR_0tag_Xtohh1000\", \"QCDCR_1tag_Xtohh1000\", \"QCDCR_2tag_Xtohh1000\",\n",
    "           \"SR_0tag_Xtohh1000\", \"SR_1tag_Xtohh1000\", \"SR_2tag_Xtohh1000\",\n",
    "           \"PreSel_0tag_Xtohh2000\", \"PreSel_1tag_Xtohh2000\", \"PreSel_2tag_Xtohh2000\",\n",
    "           \"QCDCR_0tag_Xtohh2000\", \"QCDCR_1tag_Xtohh2000\", \"QCDCR_2tag_Xtohh2000\",\n",
    "           \"SR_0tag_Xtohh2000\", \"SR_1tag_Xtohh2000\", \"SR_2tag_Xtohh2000\"]\n",
    "s_end = \"_s.root\"\n",
    "b_end = \"_b.root\"\n",
    "comp_params_path = \"comp_params.txt\"\n",
    "comp_params = get_params(comp_params_path)\n",
    "model_input_path = \"dnn_model2.csv\"\n",
    "model_input = get_params(model_input_path)\n",
    "model_name = \"model_dense.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.7060 - categorical_accuracy: 0.4688 - val_loss: 0.6771 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67705, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 0s 78us/step - loss: 0.7158 - categorical_accuracy: 0.4844 - val_loss: 0.6833 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.67705\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 0s 77us/step - loss: 0.6965 - categorical_accuracy: 0.5000 - val_loss: 0.6743 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67705 to 0.67435, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 0s 71us/step - loss: 0.7012 - categorical_accuracy: 0.5078 - val_loss: 0.6704 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67435 to 0.67040, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 0.7014 - categorical_accuracy: 0.4922 - val_loss: 0.6698 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.67040 to 0.66980, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 0.7030 - categorical_accuracy: 0.4922 - val_loss: 0.6697 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.66980 to 0.66967, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 0s 74us/step - loss: 0.7047 - categorical_accuracy: 0.4922 - val_loss: 0.6702 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66967\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 0.7007 - categorical_accuracy: 0.4922 - val_loss: 0.6702 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66967\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 0s 91us/step - loss: 0.7010 - categorical_accuracy: 0.4922 - val_loss: 0.6704 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66967\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 0.7002 - categorical_accuracy: 0.4922 - val_loss: 0.6702 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66967\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 0s 86us/step - loss: 0.6995 - categorical_accuracy: 0.4922 - val_loss: 0.6703 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66967\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 0s 66us/step - loss: 0.6980 - categorical_accuracy: 0.4922 - val_loss: 0.6720 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.66967\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 0s 77us/step - loss: 0.6987 - categorical_accuracy: 0.5547 - val_loss: 0.6763 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66967\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 0s 87us/step - loss: 0.6965 - categorical_accuracy: 0.5312 - val_loss: 0.6782 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66967\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 0s 78us/step - loss: 0.6953 - categorical_accuracy: 0.5000 - val_loss: 0.6779 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66967\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 0.6951 - categorical_accuracy: 0.5000 - val_loss: 0.6797 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66967\n",
      "Epoch 00016: early stopping\n",
      "ROC integral: 0.46217069084625906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46217069084625906"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.462\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.026 (0.066)       0.164 (0.137)      0.363 (0.145)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 8294\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 480267\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n"
     ]
    }
   ],
   "source": [
    "DNN(params[0], training[0], model_input[0], comp_params[0], model_name, configs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_2tag_Xtohh1000\n",
      "===============\n",
      "<class 'int'>\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 0s 3ms/step - loss: 0.7227 - categorical_accuracy: 0.5469 - val_loss: 0.5905 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59048, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.7193 - categorical_accuracy: 0.5234 - val_loss: 0.5944 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59048\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 0s 87us/step - loss: 0.7184 - categorical_accuracy: 0.5234 - val_loss: 0.5991 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59048\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.7189 - categorical_accuracy: 0.5234 - val_loss: 0.6043 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59048\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 0s 78us/step - loss: 0.7212 - categorical_accuracy: 0.5234 - val_loss: 0.6071 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59048\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 0s 87us/step - loss: 0.7229 - categorical_accuracy: 0.5234 - val_loss: 0.6040 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59048\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 0s 85us/step - loss: 0.7234 - categorical_accuracy: 0.5234 - val_loss: 0.5970 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59048\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.7170 - categorical_accuracy: 0.5234 - val_loss: 0.5926 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.59048\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 0s 74us/step - loss: 0.7144 - categorical_accuracy: 0.5234 - val_loss: 0.5839 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.59048 to 0.58393, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 0.7130 - categorical_accuracy: 0.5625 - val_loss: 0.5801 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.58393 to 0.58011, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 0s 86us/step - loss: 0.7089 - categorical_accuracy: 0.6562 - val_loss: 0.5786 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.58011 to 0.57865, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 0s 80us/step - loss: 0.7083 - categorical_accuracy: 0.6641 - val_loss: 0.5778 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.57865 to 0.57776, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 0s 72us/step - loss: 0.7083 - categorical_accuracy: 0.6562 - val_loss: 0.5768 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.57776 to 0.57683, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.7056 - categorical_accuracy: 0.6641 - val_loss: 0.5741 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.57683 to 0.57411, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 0s 81us/step - loss: 0.7043 - categorical_accuracy: 0.7188 - val_loss: 0.5723 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.57411 to 0.57234, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 0.7032 - categorical_accuracy: 0.7109 - val_loss: 0.5712 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.57234 to 0.57125, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 0s 76us/step - loss: 0.7013 - categorical_accuracy: 0.7188 - val_loss: 0.5716 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.57125\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 0s 99us/step - loss: 0.6987 - categorical_accuracy: 0.6719 - val_loss: 0.5741 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.57125\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 0.6965 - categorical_accuracy: 0.6328 - val_loss: 0.5804 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.57125\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 0s 92us/step - loss: 0.6981 - categorical_accuracy: 0.5625 - val_loss: 0.5887 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.57125\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 0s 92us/step - loss: 0.7011 - categorical_accuracy: 0.5469 - val_loss: 0.5952 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.57125\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 0s 91us/step - loss: 0.7105 - categorical_accuracy: 0.5391 - val_loss: 0.5915 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.57125\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 0s 104us/step - loss: 0.7021 - categorical_accuracy: 0.5469 - val_loss: 0.5765 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.57125\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 0s 95us/step - loss: 0.6903 - categorical_accuracy: 0.5781 - val_loss: 0.5677 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.57125 to 0.56769, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 0s 81us/step - loss: 0.6882 - categorical_accuracy: 0.6797 - val_loss: 0.5616 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.56769 to 0.56158, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 0s 90us/step - loss: 0.6859 - categorical_accuracy: 0.7344 - val_loss: 0.5584 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.56158 to 0.55840, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 0s 75us/step - loss: 0.6849 - categorical_accuracy: 0.7344 - val_loss: 0.5544 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.55840 to 0.55444, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 0.6851 - categorical_accuracy: 0.6406 - val_loss: 0.5513 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.55444 to 0.55131, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "128/128 [==============================] - 0s 81us/step - loss: 0.6837 - categorical_accuracy: 0.6094 - val_loss: 0.5495 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.55131 to 0.54947, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.6806 - categorical_accuracy: 0.6016 - val_loss: 0.5473 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.54947 to 0.54733, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 0s 80us/step - loss: 0.6754 - categorical_accuracy: 0.6406 - val_loss: 0.5463 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.54733 to 0.54630, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 0s 96us/step - loss: 0.6685 - categorical_accuracy: 0.6641 - val_loss: 0.5512 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.54630\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 0s 95us/step - loss: 0.6790 - categorical_accuracy: 0.6875 - val_loss: 0.5558 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.54630\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 0s 90us/step - loss: 0.6609 - categorical_accuracy: 0.6562 - val_loss: 0.5433 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.54630 to 0.54335, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.6542 - categorical_accuracy: 0.7188 - val_loss: 0.5354 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.54335 to 0.53540, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.6678 - categorical_accuracy: 0.6016 - val_loss: 0.5369 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.53540\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 0s 98us/step - loss: 0.6805 - categorical_accuracy: 0.5312 - val_loss: 0.5338 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.53540 to 0.53377, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 0s 75us/step - loss: 0.6743 - categorical_accuracy: 0.5469 - val_loss: 0.5289 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.53377 to 0.52891, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 0s 90us/step - loss: 0.6575 - categorical_accuracy: 0.6094 - val_loss: 0.5302 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.52891\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 0s 97us/step - loss: 0.6417 - categorical_accuracy: 0.6719 - val_loss: 0.5398 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.52891\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 0s 83us/step - loss: 0.6372 - categorical_accuracy: 0.6797 - val_loss: 0.5556 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.52891\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 0s 92us/step - loss: 0.6517 - categorical_accuracy: 0.6250 - val_loss: 0.5556 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.52891\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 0s 88us/step - loss: 0.6399 - categorical_accuracy: 0.6250 - val_loss: 0.5325 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.52891\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 0s 119us/step - loss: 0.6256 - categorical_accuracy: 0.7109 - val_loss: 0.5191 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.52891 to 0.51915, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 0s 75us/step - loss: 0.6264 - categorical_accuracy: 0.7031 - val_loss: 0.5147 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.51915 to 0.51474, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 0s 113us/step - loss: 0.6186 - categorical_accuracy: 0.6953 - val_loss: 0.5195 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51474\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 0s 77us/step - loss: 0.6090 - categorical_accuracy: 0.7188 - val_loss: 0.5370 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51474\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 0s 86us/step - loss: 0.6297 - categorical_accuracy: 0.6641 - val_loss: 0.5437 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51474\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 0s 92us/step - loss: 0.6119 - categorical_accuracy: 0.6719 - val_loss: 0.5162 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51474\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 0s 116us/step - loss: 0.6014 - categorical_accuracy: 0.7344 - val_loss: 0.5023 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.51474 to 0.50231, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 0s 86us/step - loss: 0.5976 - categorical_accuracy: 0.7031 - val_loss: 0.4961 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.50231 to 0.49607, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 0s 97us/step - loss: 0.6037 - categorical_accuracy: 0.6484 - val_loss: 0.4931 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.49607 to 0.49309, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 0s 85us/step - loss: 0.6059 - categorical_accuracy: 0.6406 - val_loss: 0.4896 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.49309 to 0.48963, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 0s 69us/step - loss: 0.5883 - categorical_accuracy: 0.6250 - val_loss: 0.4929 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48963\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.5700 - categorical_accuracy: 0.7344 - val_loss: 0.5199 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48963\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 0s 80us/step - loss: 0.5744 - categorical_accuracy: 0.6953 - val_loss: 0.5453 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.48963\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 0s 74us/step - loss: 0.5918 - categorical_accuracy: 0.6406 - val_loss: 0.5243 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.48963\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 0s 76us/step - loss: 0.5654 - categorical_accuracy: 0.6875 - val_loss: 0.4795 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.48963 to 0.47949, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.5595 - categorical_accuracy: 0.6875 - val_loss: 0.4727 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.47949 to 0.47265, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 0s 71us/step - loss: 0.5694 - categorical_accuracy: 0.6562 - val_loss: 0.4694 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.47265 to 0.46936, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 100us/step - loss: 0.5630 - categorical_accuracy: 0.6250 - val_loss: 0.4676 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.46936 to 0.46764, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 0s 93us/step - loss: 0.5405 - categorical_accuracy: 0.6641 - val_loss: 0.4769 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.46764\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 0s 88us/step - loss: 0.5260 - categorical_accuracy: 0.7109 - val_loss: 0.5042 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.46764\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 0s 94us/step - loss: 0.5266 - categorical_accuracy: 0.7344 - val_loss: 0.5205 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.46764\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 0s 87us/step - loss: 0.5335 - categorical_accuracy: 0.7109 - val_loss: 0.4918 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.46764\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 0s 95us/step - loss: 0.5177 - categorical_accuracy: 0.7266 - val_loss: 0.4571 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.46764 to 0.45714, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 0s 85us/step - loss: 0.5211 - categorical_accuracy: 0.6562 - val_loss: 0.4547 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.45714 to 0.45472, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.5257 - categorical_accuracy: 0.6562 - val_loss: 0.4581 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.45472\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 0s 75us/step - loss: 0.5058 - categorical_accuracy: 0.7109 - val_loss: 0.4596 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.45472\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 0s 85us/step - loss: 0.4962 - categorical_accuracy: 0.7188 - val_loss: 0.4647 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.45472\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 0s 110us/step - loss: 0.5005 - categorical_accuracy: 0.7344 - val_loss: 0.4731 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.45472\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 0s 85us/step - loss: 0.4763 - categorical_accuracy: 0.7266 - val_loss: 0.4555 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.45472\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 0s 77us/step - loss: 0.4858 - categorical_accuracy: 0.6953 - val_loss: 0.4510 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.45472 to 0.45096, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 0s 79us/step - loss: 0.4893 - categorical_accuracy: 0.6953 - val_loss: 0.4599 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.45096\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.4681 - categorical_accuracy: 0.7266 - val_loss: 0.4862 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.45096\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 0s 88us/step - loss: 0.4674 - categorical_accuracy: 0.7422 - val_loss: 0.5047 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.45096\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 0s 117us/step - loss: 0.4759 - categorical_accuracy: 0.7188 - val_loss: 0.5004 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.45096\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 0s 81us/step - loss: 0.4480 - categorical_accuracy: 0.7266 - val_loss: 0.5405 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.45096\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 0s 82us/step - loss: 0.4703 - categorical_accuracy: 0.7188 - val_loss: 0.5531 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.45096\n",
      "Epoch 80/100\n",
      "128/128 [==============================] - 0s 81us/step - loss: 0.4769 - categorical_accuracy: 0.7344 - val_loss: 0.5196 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.45096\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.4395 - categorical_accuracy: 0.7266 - val_loss: 0.4845 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.45096\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.4292 - categorical_accuracy: 0.7266 - val_loss: 0.4571 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.45096\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 0s 76us/step - loss: 0.4689 - categorical_accuracy: 0.6875 - val_loss: 0.4544 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.45096\n",
      "Epoch 00083: early stopping\n",
      "ROC integral: 0.7148147738511544\n",
      "<class 'int'>\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 0s 3ms/step - loss: 0.7229 - categorical_accuracy: 0.5234 - val_loss: 0.5846 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58464, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 0s 50us/step - loss: 0.7186 - categorical_accuracy: 0.5312 - val_loss: 0.5843 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58464 to 0.58432, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 0s 40us/step - loss: 0.7158 - categorical_accuracy: 0.5391 - val_loss: 0.5829 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58432 to 0.58285, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 0s 38us/step - loss: 0.7131 - categorical_accuracy: 0.5391 - val_loss: 0.5805 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58285 to 0.58055, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 0s 40us/step - loss: 0.7102 - categorical_accuracy: 0.5703 - val_loss: 0.5780 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58055 to 0.57800, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 0s 49us/step - loss: 0.7076 - categorical_accuracy: 0.5938 - val_loss: 0.5748 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.57800 to 0.57484, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 0s 53us/step - loss: 0.7048 - categorical_accuracy: 0.6406 - val_loss: 0.5738 - val_categorical_accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss improved from 0.57484 to 0.57382, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 0s 52us/step - loss: 0.7022 - categorical_accuracy: 0.6250 - val_loss: 0.5730 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.57382 to 0.57301, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 0.6997 - categorical_accuracy: 0.5938 - val_loss: 0.5708 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.57301 to 0.57082, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 0s 37us/step - loss: 0.6968 - categorical_accuracy: 0.6250 - val_loss: 0.5673 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.57082 to 0.56731, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 0.6932 - categorical_accuracy: 0.6406 - val_loss: 0.5626 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.56731 to 0.56263, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 0s 40us/step - loss: 0.6894 - categorical_accuracy: 0.6719 - val_loss: 0.5585 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56263 to 0.55853, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.6858 - categorical_accuracy: 0.7031 - val_loss: 0.5546 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.55853 to 0.55461, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 0s 37us/step - loss: 0.6817 - categorical_accuracy: 0.6953 - val_loss: 0.5513 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.55461 to 0.55135, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 0s 49us/step - loss: 0.6768 - categorical_accuracy: 0.7031 - val_loss: 0.5495 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.55135 to 0.54954, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 0s 49us/step - loss: 0.6721 - categorical_accuracy: 0.6953 - val_loss: 0.5474 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.54954 to 0.54743, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 0s 45us/step - loss: 0.6673 - categorical_accuracy: 0.7031 - val_loss: 0.5435 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.54743 to 0.54345, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 0s 53us/step - loss: 0.6622 - categorical_accuracy: 0.7031 - val_loss: 0.5387 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.54345 to 0.53872, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 0s 51us/step - loss: 0.6567 - categorical_accuracy: 0.7031 - val_loss: 0.5349 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.53872 to 0.53486, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 0.6508 - categorical_accuracy: 0.6875 - val_loss: 0.5316 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.53486 to 0.53164, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 0.6445 - categorical_accuracy: 0.7188 - val_loss: 0.5276 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.53164 to 0.52764, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 0s 38us/step - loss: 0.6378 - categorical_accuracy: 0.7031 - val_loss: 0.5222 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.52764 to 0.52215, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 0.6307 - categorical_accuracy: 0.6797 - val_loss: 0.5175 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.52215 to 0.51745, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 0s 43us/step - loss: 0.6231 - categorical_accuracy: 0.6875 - val_loss: 0.5136 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.51745 to 0.51358, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 0s 38us/step - loss: 0.6152 - categorical_accuracy: 0.6875 - val_loss: 0.5083 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.51358 to 0.50826, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 0s 36us/step - loss: 0.6067 - categorical_accuracy: 0.6953 - val_loss: 0.5031 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.50826 to 0.50307, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.5979 - categorical_accuracy: 0.6875 - val_loss: 0.4984 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.50307 to 0.49844, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 0.5887 - categorical_accuracy: 0.6953 - val_loss: 0.4928 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.49844 to 0.49277, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 0s 48us/step - loss: 0.5791 - categorical_accuracy: 0.6953 - val_loss: 0.4880 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.49277 to 0.48802, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 0s 40us/step - loss: 0.5692 - categorical_accuracy: 0.7031 - val_loss: 0.4840 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48802 to 0.48395, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 0.5589 - categorical_accuracy: 0.7109 - val_loss: 0.4781 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.48395 to 0.47812, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 0.5483 - categorical_accuracy: 0.7109 - val_loss: 0.4746 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.47812 to 0.47457, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 0.5375 - categorical_accuracy: 0.7188 - val_loss: 0.4697 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.47457 to 0.46972, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.5264 - categorical_accuracy: 0.7188 - val_loss: 0.4664 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.46972 to 0.46635, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 0s 52us/step - loss: 0.5153 - categorical_accuracy: 0.7266 - val_loss: 0.4650 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.46635 to 0.46499, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 0s 42us/step - loss: 0.5042 - categorical_accuracy: 0.7188 - val_loss: 0.4608 - val_categorical_accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss improved from 0.46499 to 0.46080, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 0s 47us/step - loss: 0.4933 - categorical_accuracy: 0.7266 - val_loss: 0.4665 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.46080\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 0.4826 - categorical_accuracy: 0.7266 - val_loss: 0.4515 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.46080 to 0.45146, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 0s 40us/step - loss: 0.4732 - categorical_accuracy: 0.7188 - val_loss: 0.4848 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.45146\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 0s 50us/step - loss: 0.4659 - categorical_accuracy: 0.7266 - val_loss: 0.4572 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.45146\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.4549 - categorical_accuracy: 0.7266 - val_loss: 0.4589 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.45146\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.4470 - categorical_accuracy: 0.7266 - val_loss: 0.4915 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.45146\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 0s 52us/step - loss: 0.4407 - categorical_accuracy: 0.7344 - val_loss: 0.4748 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.45146\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.4312 - categorical_accuracy: 0.7266 - val_loss: 0.4670 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.45146\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 0.4260 - categorical_accuracy: 0.7266 - val_loss: 0.5012 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.45146\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.4181 - categorical_accuracy: 0.7344 - val_loss: 0.5045 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.45146\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 0s 49us/step - loss: 0.4113 - categorical_accuracy: 0.7344 - val_loss: 0.4864 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.45146\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 0s 53us/step - loss: 0.4065 - categorical_accuracy: 0.7266 - val_loss: 0.5143 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45146\n",
      "Epoch 00048: early stopping\n",
      "ROC integral: 0.7413471111412634\n",
      "<class 'int'>\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 0s 3ms/step - loss: 0.7495 - categorical_accuracy: 0.5234 - val_loss: 0.6218 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62179, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 0s 88us/step - loss: 0.7279 - categorical_accuracy: 0.5234 - val_loss: 0.5975 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62179 to 0.59748, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 0s 76us/step - loss: 0.7196 - categorical_accuracy: 0.5234 - val_loss: 0.5857 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59748 to 0.58574, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 0s 82us/step - loss: 0.7194 - categorical_accuracy: 0.4609 - val_loss: 0.5837 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58574 to 0.58365, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 0s 76us/step - loss: 0.7141 - categorical_accuracy: 0.5703 - val_loss: 0.5862 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58365\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 0s 98us/step - loss: 0.7131 - categorical_accuracy: 0.5469 - val_loss: 0.5893 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58365\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 0s 102us/step - loss: 0.7130 - categorical_accuracy: 0.5312 - val_loss: 0.5929 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58365\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 0s 95us/step - loss: 0.7167 - categorical_accuracy: 0.5234 - val_loss: 0.5921 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58365\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 0s 100us/step - loss: 0.7126 - categorical_accuracy: 0.5312 - val_loss: 0.5856 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.58365\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 0s 101us/step - loss: 0.7105 - categorical_accuracy: 0.5547 - val_loss: 0.5816 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.58365 to 0.58159, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 0s 95us/step - loss: 0.7094 - categorical_accuracy: 0.7500 - val_loss: 0.5791 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.58159 to 0.57914, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 0s 94us/step - loss: 0.7090 - categorical_accuracy: 0.5859 - val_loss: 0.5765 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.57914 to 0.57653, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 0s 99us/step - loss: 0.7084 - categorical_accuracy: 0.4922 - val_loss: 0.5734 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.57653 to 0.57338, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 0s 110us/step - loss: 0.7125 - categorical_accuracy: 0.4844 - val_loss: 0.5713 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.57338 to 0.57132, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 0s 104us/step - loss: 0.7144 - categorical_accuracy: 0.4844 - val_loss: 0.5711 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.57132 to 0.57110, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.6943 - categorical_accuracy: 0.49 - 0s 96us/step - loss: 0.7098 - categorical_accuracy: 0.4766 - val_loss: 0.5735 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.57110\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 90us/step - loss: 0.7025 - categorical_accuracy: 0.5469 - val_loss: 0.5795 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.57110\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 0s 106us/step - loss: 0.6988 - categorical_accuracy: 0.5938 - val_loss: 0.5905 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.57110\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 0s 92us/step - loss: 0.7018 - categorical_accuracy: 0.5312 - val_loss: 0.6041 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.57110\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 0s 87us/step - loss: 0.7079 - categorical_accuracy: 0.5234 - val_loss: 0.6140 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.57110\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 0s 110us/step - loss: 0.7149 - categorical_accuracy: 0.5234 - val_loss: 0.6193 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.57110\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 0s 95us/step - loss: 0.7201 - categorical_accuracy: 0.5234 - val_loss: 0.6119 - val_categorical_accuracy: 0.4062\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.57110\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 0s 88us/step - loss: 0.7103 - categorical_accuracy: 0.5234 - val_loss: 0.5922 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.57110\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 0s 100us/step - loss: 0.6941 - categorical_accuracy: 0.5312 - val_loss: 0.5764 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.57110\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 0s 96us/step - loss: 0.6928 - categorical_accuracy: 0.6250 - val_loss: 0.5652 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.57110 to 0.56520, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 0s 102us/step - loss: 0.7062 - categorical_accuracy: 0.5000 - val_loss: 0.5629 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.56520 to 0.56294, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 0s 90us/step - loss: 0.7100 - categorical_accuracy: 0.4766 - val_loss: 0.5626 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.56294 to 0.56256, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 0s 84us/step - loss: 0.7122 - categorical_accuracy: 0.4766 - val_loss: 0.5622 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.56256 to 0.56225, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 0s 103us/step - loss: 0.7095 - categorical_accuracy: 0.4844 - val_loss: 0.5616 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.56225 to 0.56160, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 0s 79us/step - loss: 0.7078 - categorical_accuracy: 0.4844 - val_loss: 0.5605 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.56160 to 0.56055, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 0s 86us/step - loss: 0.7003 - categorical_accuracy: 0.4766 - val_loss: 0.5612 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.56055\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 0s 121us/step - loss: 0.6954 - categorical_accuracy: 0.5312 - val_loss: 0.5650 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.56055\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 0s 127us/step - loss: 0.6859 - categorical_accuracy: 0.6562 - val_loss: 0.5692 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.56055\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 0s 101us/step - loss: 0.6824 - categorical_accuracy: 0.7344 - val_loss: 0.5773 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.56055\n",
      "Epoch 35/200\n",
      "128/128 [==============================] - 0s 109us/step - loss: 0.6813 - categorical_accuracy: 0.5859 - val_loss: 0.5877 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.56055\n",
      "Epoch 36/200\n",
      "128/128 [==============================] - 0s 107us/step - loss: 0.6849 - categorical_accuracy: 0.5469 - val_loss: 0.5980 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.56055\n",
      "Epoch 37/200\n",
      "128/128 [==============================] - 0s 87us/step - loss: 0.6890 - categorical_accuracy: 0.5391 - val_loss: 0.6018 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.56055\n",
      "Epoch 38/200\n",
      "128/128 [==============================] - 0s 92us/step - loss: 0.6910 - categorical_accuracy: 0.5391 - val_loss: 0.5908 - val_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.56055\n",
      "Epoch 39/200\n",
      "128/128 [==============================] - 0s 93us/step - loss: 0.6866 - categorical_accuracy: 0.5547 - val_loss: 0.5720 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.56055\n",
      "Epoch 40/200\n",
      "128/128 [==============================] - 0s 103us/step - loss: 0.6751 - categorical_accuracy: 0.6016 - val_loss: 0.5633 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.56055\n",
      "Epoch 00040: early stopping\n",
      "ROC integral: 0.7189783991016372\n",
      "<class 'int'>\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 0s 4ms/step - loss: 0.7159 - categorical_accuracy: 0.5469 - val_loss: 0.5776 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57757, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.7143 - categorical_accuracy: 0.5078 - val_loss: 0.5784 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57757\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 0s 49us/step - loss: 0.7116 - categorical_accuracy: 0.6641 - val_loss: 0.5808 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57757\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.7097 - categorical_accuracy: 0.5703 - val_loss: 0.5808 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57757\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.7079 - categorical_accuracy: 0.5703 - val_loss: 0.5772 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.57757 to 0.57720, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 0s 43us/step - loss: 0.7055 - categorical_accuracy: 0.5703 - val_loss: 0.5725 - val_categorical_accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss improved from 0.57720 to 0.57246, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.7031 - categorical_accuracy: 0.6328 - val_loss: 0.5676 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.57246 to 0.56759, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.7010 - categorical_accuracy: 0.7266 - val_loss: 0.5652 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.56759 to 0.56523, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 0s 62us/step - loss: 0.6992 - categorical_accuracy: 0.7734 - val_loss: 0.5644 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.56523 to 0.56442, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 0s 66us/step - loss: 0.6969 - categorical_accuracy: 0.7734 - val_loss: 0.5639 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.56442 to 0.56385, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 0s 40us/step - loss: 0.6948 - categorical_accuracy: 0.6562 - val_loss: 0.5620 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.56385 to 0.56200, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.6924 - categorical_accuracy: 0.6562 - val_loss: 0.5591 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56200 to 0.55910, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.6898 - categorical_accuracy: 0.6719 - val_loss: 0.5558 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.55910 to 0.55578, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 0s 42us/step - loss: 0.6872 - categorical_accuracy: 0.7031 - val_loss: 0.5524 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.55578 to 0.55239, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 0s 54us/step - loss: 0.6841 - categorical_accuracy: 0.7656 - val_loss: 0.5494 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.55239 to 0.54943, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 0s 57us/step - loss: 0.6806 - categorical_accuracy: 0.7656 - val_loss: 0.5469 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.54943 to 0.54694, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 0s 52us/step - loss: 0.6771 - categorical_accuracy: 0.7344 - val_loss: 0.5442 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.54694 to 0.54416, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.6736 - categorical_accuracy: 0.7500 - val_loss: 0.5410 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.54416 to 0.54099, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 0s 76us/step - loss: 0.6695 - categorical_accuracy: 0.7656 - val_loss: 0.5372 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.54099 to 0.53720, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 0s 77us/step - loss: 0.6655 - categorical_accuracy: 0.7734 - val_loss: 0.5329 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.53720 to 0.53290, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 0s 66us/step - loss: 0.6607 - categorical_accuracy: 0.7734 - val_loss: 0.5284 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.53290 to 0.52837, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 0s 44us/step - loss: 0.6559 - categorical_accuracy: 0.7734 - val_loss: 0.5238 - val_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.52837 to 0.52381, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 0s 39us/step - loss: 0.6508 - categorical_accuracy: 0.7734 - val_loss: 0.5185 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.52381 to 0.51855, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 0s 43us/step - loss: 0.6452 - categorical_accuracy: 0.7734 - val_loss: 0.5129 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.51855 to 0.51290, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.6392 - categorical_accuracy: 0.7578 - val_loss: 0.5076 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.51290 to 0.50761, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 0s 57us/step - loss: 0.6328 - categorical_accuracy: 0.7578 - val_loss: 0.5029 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.50761 to 0.50289, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 0s 44us/step - loss: 0.6262 - categorical_accuracy: 0.7578 - val_loss: 0.4975 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.50289 to 0.49745, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.6190 - categorical_accuracy: 0.7578 - val_loss: 0.4906 - val_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.49745 to 0.49065, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 0s 59us/step - loss: 0.6112 - categorical_accuracy: 0.7578 - val_loss: 0.4834 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.49065 to 0.48338, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 0s 52us/step - loss: 0.6031 - categorical_accuracy: 0.7500 - val_loss: 0.4775 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48338 to 0.47751, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 0s 48us/step - loss: 0.5947 - categorical_accuracy: 0.7500 - val_loss: 0.4718 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.47751 to 0.47181, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 0s 78us/step - loss: 0.5859 - categorical_accuracy: 0.7500 - val_loss: 0.4651 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.47181 to 0.46508, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.5767 - categorical_accuracy: 0.7656 - val_loss: 0.4570 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.46508 to 0.45700, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 0s 85us/step - loss: 0.5672 - categorical_accuracy: 0.7422 - val_loss: 0.4509 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.45700 to 0.45094, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 35/200\n",
      "128/128 [==============================] - 0s 49us/step - loss: 0.5574 - categorical_accuracy: 0.7422 - val_loss: 0.4461 - val_categorical_accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss improved from 0.45094 to 0.44610, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 36/200\n",
      "128/128 [==============================] - 0s 44us/step - loss: 0.5472 - categorical_accuracy: 0.7578 - val_loss: 0.4394 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.44610 to 0.43945, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 37/200\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.5368 - categorical_accuracy: 0.7500 - val_loss: 0.4323 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.43945 to 0.43232, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 38/200\n",
      "128/128 [==============================] - 0s 64us/step - loss: 0.5263 - categorical_accuracy: 0.7500 - val_loss: 0.4301 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.43232 to 0.43015, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 39/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.5154 - categorical_accuracy: 0.7500 - val_loss: 0.4233 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.43015 to 0.42331, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 40/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.5046 - categorical_accuracy: 0.7578 - val_loss: 0.4206 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.42331 to 0.42056, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 41/200\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.4938 - categorical_accuracy: 0.7578 - val_loss: 0.4191 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.42056 to 0.41911, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 42/200\n",
      "128/128 [==============================] - 0s 54us/step - loss: 0.4830 - categorical_accuracy: 0.7578 - val_loss: 0.4130 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.41911 to 0.41299, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 43/200\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.4725 - categorical_accuracy: 0.7734 - val_loss: 0.4200 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.41299\n",
      "Epoch 44/200\n",
      "128/128 [==============================] - 0s 64us/step - loss: 0.4624 - categorical_accuracy: 0.7578 - val_loss: 0.4076 - val_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.41299 to 0.40762, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 45/200\n",
      "128/128 [==============================] - 0s 63us/step - loss: 0.4533 - categorical_accuracy: 0.7656 - val_loss: 0.4262 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.40762\n",
      "Epoch 46/200\n",
      "128/128 [==============================] - 0s 79us/step - loss: 0.4443 - categorical_accuracy: 0.7578 - val_loss: 0.4125 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.40762\n",
      "Epoch 47/200\n",
      "128/128 [==============================] - 0s 86us/step - loss: 0.4349 - categorical_accuracy: 0.7656 - val_loss: 0.4205 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.40762\n",
      "Epoch 48/200\n",
      "128/128 [==============================] - 0s 74us/step - loss: 0.4260 - categorical_accuracy: 0.7656 - val_loss: 0.4344 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.40762\n",
      "Epoch 49/200\n",
      "128/128 [==============================] - 0s 75us/step - loss: 0.4189 - categorical_accuracy: 0.7578 - val_loss: 0.4213 - val_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.40762\n",
      "Epoch 50/200\n",
      "128/128 [==============================] - 0s 82us/step - loss: 0.4118 - categorical_accuracy: 0.7656 - val_loss: 0.4438 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.40762\n",
      "Epoch 51/200\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.4037 - categorical_accuracy: 0.7578 - val_loss: 0.4401 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.40762\n",
      "Epoch 52/200\n",
      "128/128 [==============================] - 0s 74us/step - loss: 0.3959 - categorical_accuracy: 0.7656 - val_loss: 0.4398 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.40762\n",
      "Epoch 53/200\n",
      "128/128 [==============================] - 0s 63us/step - loss: 0.3895 - categorical_accuracy: 0.7656 - val_loss: 0.4659 - val_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.40762\n",
      "Epoch 54/200\n",
      "128/128 [==============================] - 0s 78us/step - loss: 0.3835 - categorical_accuracy: 0.7578 - val_loss: 0.4487 - val_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.40762\n",
      "Epoch 00054: early stopping\n",
      "ROC integral: 0.7727163747529888\n",
      "best parameters: H:!V:VarTransform=N_AllClasses:FilenameModel=model_dense.h5:NumEpochs=200:BatchSize=200:TriesEarlyStopping=10\n",
      "best training: nTrain_Signal=80%:nTrain_Background=80%:SplitMode=Random:NormMode=NumEvents:!V\n",
      "best model: numero_HL=3, neurons=64, neurons_LF=2, k_init=glorot_normal, activation_IL=sigmoid, activation_HL=relu, activation_FL=softmax\n",
      "ROC integral: 0.7727163747529888\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.715\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.041 (0.295)       0.284 (0.692)      0.619 (0.960)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.741\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.031 (0.187)       0.251 (0.766)      0.707 (0.930)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.719\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.025 (0.247)       0.215 (0.770)      0.619 (0.939)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.773\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.029 (0.217)       0.321 (0.767)      0.747 (0.969)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 5526\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 244\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 5526\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 244\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 5526\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 244\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 5526\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_2tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_2tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 244\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n"
     ]
    }
   ],
   "source": [
    "param_opt(configs[8], params, training, model_input, comp_params, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weights\n",
    "def v_out(tree, not_cons, variables, reader):\n",
    "    \n",
    "    h = ROOT.TH1D(\"\",\"\",60,-1,1)\n",
    "    \n",
    "    nevt = tree.GetEntries()\n",
    "\n",
    "    vout = np.arange(nevt, dtype='float').reshape(1, nevt)\n",
    "\n",
    "    for ievt, entry in enumerate(tree):\n",
    "        i = 0    \n",
    "        for branch in tree.GetListOfBranches():\n",
    "            name = branch.GetName()\n",
    "            if name in not_cons:\n",
    "                continue\n",
    "            variables[i][0] = getattr(entry,name)\n",
    "            i += 1\n",
    "\n",
    "        vout[0,ievt] = reader.EvaluateMVA(methodName)\n",
    "        h.Fill(vout[0,ievt])\n",
    "    \n",
    "    return h, vout, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_report_pred(background, signal, data, sep):\n",
    "    background = list(background[0])\n",
    "    signal = list(signal[0])\n",
    "    data = list(data[0])\n",
    "    bakg_t = [0]*len(background)\n",
    "    signal_t = [1]*len(signal)\n",
    "    y_predicted = background + signal\n",
    "    y_test = bakg_t + signal_t\n",
    "    for i in range(len(y_predicted)):\n",
    "        if (y_predicted[i] < sep):\n",
    "            y_predicted[i] = 0\n",
    "        else:\n",
    "            y_predicted[i] = 1\n",
    "    for j in range(len(data)):\n",
    "        if (data[j] < sep):\n",
    "            data[j] = 0\n",
    "        else:\n",
    "            data[j] = 1\n",
    "    print(classification_report(y_test, y_predicted, target_names=[\"background\", \"signal\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config, type_signal, methodName, sep):\n",
    "    reader = TMVA.Reader( \"!Color:!Silent\" )\n",
    "    \n",
    "    dataPath = config + \"_\" + type_signal + \"_d.root\"\n",
    "    bkgPath = config + \"_\" + type_signal + \"_b.root\"\n",
    "    sigPath = config + \"_\" + type_signal + \"_s.root\"\n",
    "    \n",
    "    print(dataPath)\n",
    "    \n",
    "    dataFile = ROOT.TFile(dataPath)\n",
    "    bkgFile = ROOT.TFile(bkgPath)\n",
    "    sigFile = ROOT.TFile(sigPath)\n",
    "\n",
    "    dataTree = dataFile.Nominal\n",
    "    bkgTree = bkgFile.Nominal\n",
    "    sigTree = sigFile.Nominal\n",
    "    \n",
    "    # Add Variables: We add variables to the reader exactly in the same way we did for the **DataLoader** during the training\n",
    "    # We need to specify the address of the variable in order to pass it to TMVA when we iterate on the TTree\n",
    "    variables = []\n",
    "    i = 0\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "    \n",
    "    for branch in dataTree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        aux = array('f',[0])\n",
    "        variables.append(aux)\n",
    "        reader.AddVariable(branch.GetName(),variables[i])\n",
    "        i = i+1\n",
    "    \n",
    "    # Setup Classifiers: We set up the classifiers by reading the input weights from the appropriate files\n",
    "    # The file is stored for example as *dataset/weights/TMVAClassification_BDT.weights.xml\n",
    "    weightfile = \"dataset/weights/TMVA_Higgs_Classification_\" + config + \"_\" + type_signal + \"_\" + methodName + \".weights.xml\"\n",
    "    name = ROOT.TString(methodName)\n",
    "    reader.BookMVA( name, weightfile )\n",
    "    \n",
    "    # We iterate on the input event in the given TTree. We provide as input first the background tree \n",
    "    # We return the output results for the various methods in big numpy array [ number of methods x \n",
    "    # number of events]\n",
    "    # We also fill an histogram for each method.\n",
    "    # Note that is important to fill the arrays with the tree entries in order to pass the values to \n",
    "    # the TMVA::Reader\n",
    "    hd, d_vout, variables = v_out(dataTree, not_cons, variables, reader)\n",
    "    hs, s_vout, variables = v_out(sigTree, not_cons, variables, reader)\n",
    "    hb, b_vout, variables = v_out(bkgTree, not_cons, variables, reader)\n",
    "    \n",
    "    print(\"Signal size:\", len(s_vout[0]))\n",
    "    print(\"Background size:\", len(b_vout[0]))\n",
    "    \n",
    "    # Classification report\n",
    "    prediction = gen_report_pred(b_vout, s_vout, d_vout, sep)\n",
    "    \n",
    "    print(\"Prediction size:\", len(prediction))\n",
    "    \n",
    "    # Histogram\n",
    "    c1 = ROOT.TCanvas(\"c1\", \"c1\")\n",
    "    c1.cd()\n",
    "    hb.Draw()\n",
    "    hs.SetLineColor(ROOT.kRed)\n",
    "    hd.SetFillColor(ROOT.kGreen)\n",
    "    hb.SetFillColor(ROOT.kBlue)\n",
    "    hb.Draw()\n",
    "    hs.Draw('Same')\n",
    "    hd.Draw('Same')\n",
    "    hb.SetTitle(\"background\")\n",
    "    hs.SetTitle(\"signal\")\n",
    "    hd.SetTitle(\"data\")\n",
    "    c1.BuildLegend()\n",
    "        \n",
    "    img_file = config + \"_\" + type_signal + \"_\" + methodName + \"_hist.png\"\n",
    "    c1.SaveAs(img_file)\n",
    "    \n",
    "    dataFile.Close()\n",
    "    sigFile.Close()\n",
    "    bkgFile.Close()\n",
    "    \n",
    "    display(Image(filename=img_file)) \n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_1tag_Xtohh1000_d.root\n",
      "Signal size: 3553\n",
      "Background size: 9262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.89      0.26      0.40      9262\n",
      "      signal       0.32      0.92      0.48      3553\n",
      "\n",
      "    accuracy                           0.44     12815\n",
      "   macro avg       0.61      0.59      0.44     12815\n",
      "weighted avg       0.74      0.44      0.42     12815\n",
      "\n",
      "Prediction size: 81\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: c1\n",
      "Info in <TCanvas::Print>: png file SR_1tag_Xtohh1000_Keras_Dense_hist.png has been created\n"
     ]
    }
   ],
   "source": [
    "config = \"SR_1tag\"\n",
    "type_signal = \"Xtohh1000\"\n",
    "methodName = \"Keras_Dense\"\n",
    "sep = 0.5\n",
    "asd = predict(config, type_signal, methodName, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
