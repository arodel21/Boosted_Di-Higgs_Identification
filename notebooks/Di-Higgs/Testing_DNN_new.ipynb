{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TMVA Classification Using Deep Neural Networks</center>\n",
    "\n",
    "In this notebook we still classify di-Higgs new data with Deep Neural Networks meethod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA, TTree\n",
    "import pandas as pd\n",
    "\n",
    "ROOT.TMVA.Tools.Instance()\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize()\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by region.\n",
    "\n",
    "This function will let you filter your dataset by region. It's known that SR_1tag is very signal poor, while SR_2tag has a lot a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_region(file, region, signal):\n",
    "    oldfile = ROOT.TFile(file)\n",
    "    oldtree = oldfile.Nominal\n",
    "    signal_file = ROOT.TFile(region+\"_\"+signal+\"_s.root\",\"recreate\")\n",
    "    signal_tree = oldtree.CloneTree(0)\n",
    "    backg_file = ROOT.TFile(region+\"_\"+signal+\"_b.root\",\"recreate\")\n",
    "    backg_tree = oldtree.CloneTree(0)\n",
    "    data_file = ROOT.TFile(region+\"_\"+signal+\"_d.root\",\"recreate\")\n",
    "    data_tree = oldtree.CloneTree(0)\n",
    "    for entry in oldtree:\n",
    "        if (entry.m_region == region):\n",
    "            if (entry.sample == \"data\"):\n",
    "                data_tree.Fill()\n",
    "            elif (entry.sample == \"Xtohh1000\"): #signal\n",
    "                signal_tree.Fill()\n",
    "            else:\n",
    "                backg_tree.Fill()\n",
    "    signal_tree.AutoSave()   \n",
    "    backg_tree.AutoSave()\n",
    "    data_tree.AutoSave()\n",
    "    return signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file\n",
    "\n",
    "#Use as\n",
    "#tree, file = filter_region(\"data.root\", \"SR_1tag\", \"small.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory and Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.root has unlabeled data points (called data) and fakes points. For the background training we'll use only the fakes points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file = filter_region(\"all_1000.root\", \"SR_1tag\", \"Xtohh1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFile = ROOT.TFile.Open(\"Higgs_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "# Factory\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                      \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "#signal_tree.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input data abd variables \n",
    "\n",
    "We add first the signal and background trees in the data loader and then we\n",
    "define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "We have two kinds of signals and for the training we have to use only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree    ( signal_tree )\n",
    "loader.AddBackgroundTree( backg_tree )\n",
    "loader.SetSignalWeightExpression(\"EventWeight\")\n",
    "\n",
    "not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "## Define input variables \n",
    "for branch in backg_tree.GetListOfBranches():\n",
    "    if branch.GetName() in not_cons:\n",
    "        continue\n",
    "    loader.AddVariable(branch.GetName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Setup the DataLoader by splitting events in training and test samples. \n",
    "Here we use a random split and a fixed number of training and test events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "loader.PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=2000:nTrain_Background=800:SplitMode=Random:\"\n",
    "                                   \"NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='sigmoid', input_dim=10))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy',])\n",
    "\n",
    "# Store model to file\n",
    "model.save('model_dense.h5')\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ROOT.TMVA::MethodPyKeras object (\"Keras_Dense\") at 0x8331c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, 'Keras_Dense',\n",
    "        'H:!V:VarTransform=G:FilenameModel=model_dense.h5:'+\\\n",
    "        'NumEpochs=10:BatchSize=16:TriesEarlyStopping=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/lib/python3.5/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2800 samples, validate on 10015 samples\n",
      "Epoch 1/10\n",
      "2800/2800 [==============================] - 3s 916us/step - loss: 0.4916 - categorical_accuracy: 0.7704 - val_loss: 0.7297 - val_categorical_accuracy: 0.6251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72968, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "2800/2800 [==============================] - 2s 537us/step - loss: 0.4324 - categorical_accuracy: 0.8132 - val_loss: 0.6737 - val_categorical_accuracy: 0.6734\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72968 to 0.67373, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/10\n",
      "2800/2800 [==============================] - 1s 514us/step - loss: 0.4220 - categorical_accuracy: 0.8236 - val_loss: 0.8488 - val_categorical_accuracy: 0.5835\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.67373\n",
      "Epoch 4/10\n",
      "2800/2800 [==============================] - 1s 513us/step - loss: 0.4103 - categorical_accuracy: 0.8300 - val_loss: 0.5878 - val_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67373 to 0.58783, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/10\n",
      "2800/2800 [==============================] - 1s 497us/step - loss: 0.4094 - categorical_accuracy: 0.8275 - val_loss: 0.6632 - val_categorical_accuracy: 0.6814\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58783\n",
      "Epoch 6/10\n",
      "2800/2800 [==============================] - 1s 515us/step - loss: 0.3962 - categorical_accuracy: 0.8382 - val_loss: 0.8446 - val_categorical_accuracy: 0.5588\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58783\n",
      "Epoch 7/10\n",
      "2800/2800 [==============================] - 1s 503us/step - loss: 0.4047 - categorical_accuracy: 0.8321 - val_loss: 0.5728 - val_categorical_accuracy: 0.7120\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.58783 to 0.57280, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 8/10\n",
      "2800/2800 [==============================] - 1s 506us/step - loss: 0.3915 - categorical_accuracy: 0.8357 - val_loss: 0.8756 - val_categorical_accuracy: 0.5772\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57280\n",
      "Epoch 9/10\n",
      "2800/2800 [==============================] - 2s 560us/step - loss: 0.3904 - categorical_accuracy: 0.8396 - val_loss: 0.8009 - val_categorical_accuracy: 0.5875\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.57280\n",
      "Epoch 10/10\n",
      "2800/2800 [==============================] - 1s 515us/step - loss: 0.3818 - categorical_accuracy: 0.8418 - val_loss: 0.6107 - val_categorical_accuracy: 0.6873\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.57280\n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Model\n",
    "\n",
    "Here we test all methods using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.TestAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Here we evaluate all methods and compare their performances, computing efficiencies, ROC curves etc.. using both training and tetsing data sets. Several histograms are produced which can be examined with the TMVAGui or directly using the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.877\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.066 (0.062)       0.600 (0.457)      0.901 (0.903)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(file):\n",
    "    params = []\n",
    "    first = 1\n",
    "    with open(file, 'r') as fp:\n",
    "        line = fp.readline().rstrip()\n",
    "        while line:\n",
    "            if (file.split('.')[1] == 'csv' and first):\n",
    "                first = 0\n",
    "                line = fp.readline().rstrip()\n",
    "                continue\n",
    "            params.append(line)\n",
    "            line = fp.readline().rstrip()       \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(params, training, model_input, comp_params, model_name, config):\n",
    "    \n",
    "    output_file = config+\"_DNN_Classification.root\"\n",
    "    signal_file = config+\"_s.root\"\n",
    "    backg_file = config+\"_b.root\"\n",
    "    \n",
    "    signal_input = ROOT.TFile(signal_file)\n",
    "    signal_tree = signal_input.Nominal\n",
    "    \n",
    "    backg_input = ROOT.TFile(backg_file)\n",
    "    backg_tree = backg_input.Nominal\n",
    "    \n",
    "    outputFile = ROOT.TFile.Open(output_file, \"RECREATE\")\n",
    "\n",
    "    # Factory\n",
    "    factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                          \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "    loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "    ### global event weights per tree (see below for setting event-wise weights)\n",
    "    #signalWeight     = 1.0\n",
    "    #backgroundWeight = 1.0\n",
    "\n",
    "    ### You can add an arbitrary number of signal or background trees\n",
    "    loader.AddSignalTree    ( signal_tree )\n",
    "    loader.AddBackgroundTree( backg_tree )\n",
    "    loader.SetSignalWeightExpression(\"EventWeight\")\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "    ## Define input variables \n",
    "    for branch in backg_tree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        loader.AddVariable(branch.GetName())\n",
    "        \n",
    "    mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "    mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "    loader.PrepareTrainingAndTestTree(mycuts, mycutb, training)\n",
    "    \n",
    "    # Model structure\n",
    "    \n",
    "    comp_params = comp_params.rstrip()\n",
    "    comp_params = comp_params.split(',')\n",
    "    loss = comp_params[0]\n",
    "    \n",
    "    comp_params.remove(loss)\n",
    "    metrics = comp_params\n",
    "    \n",
    "    model = Sequential()\n",
    "    model_input = model_input.rstrip()\n",
    "    model_input = model_input.split(',')\n",
    "    \n",
    "    hidden_l = int(model_input[0])\n",
    "    neurons = int(model_input[1])\n",
    "    neurons_LF = int(model_input[2])\n",
    "    k_init = model_input[3]\n",
    "    activation_IL = model_input[4]\n",
    "    activation_HL = model_input[5]\n",
    "    activation_FL = model_input[6]\n",
    "    \n",
    "    print(type(neurons))\n",
    "    \n",
    "    model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_IL, input_dim=10))\n",
    "    for h in range(hidden_l):\n",
    "        model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_HL))\n",
    "        \n",
    "    model.add(Dense(neurons_LF, kernel_initializer=k_init, activation=activation_FL))\n",
    "    \n",
    "    # Set loss and optimizer\n",
    "    model.compile(loss=loss, optimizer=Adam(), metrics=metrics)\n",
    "    # Store model to file\n",
    "    model.save(model_name)\n",
    "    # Print summary of model\n",
    "    model.summary()\n",
    "    \n",
    "    ## DNN method\n",
    "    factory.BookMethod(loader,ROOT.TMVA.Types.kPyKeras, \"Keras_Dense\", params)\n",
    "        \n",
    "    factory.TrainAllMethods()\n",
    "    \n",
    "    factory.TestAllMethods()\n",
    "    \n",
    "    factory.EvaluateAllMethods()\n",
    "    \n",
    "    c1 = factory.GetROCCurve(loader)\n",
    "    #c1.Draw()\n",
    "    \n",
    "    integ = factory.GetROCIntegral(loader, \"Keras_Dense\")\n",
    "    \n",
    "    print(\"ROC integral:\", integ)\n",
    "    \n",
    "    outputFile.Close()\n",
    "    signal_input.Close()\n",
    "    backg_input.Close()\n",
    "    \n",
    "    return integ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_combs_params(file_params, file_training, file_model, comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics):\n",
    "    comb_params = list(itertools.product(arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics))\n",
    "    with open(file_params, 'w') as params, open(file_training, 'w') as training, open(file_model, 'w') as model, open(comp_params, 'w') as comp_p:\n",
    "        model.write(\"number_HL,neurons,neurons_LF,k_init,activation_IL,activation_HL,activation_FL\\n\")\n",
    "        for cp in comb_params:\n",
    "            string1 = \"H:!V:VarTransform=N_AllClasses:FilenameModel=\"+model_name+\":NumEpochs=\"+str(cp[0])+\":BatchSize=\"+str(cp[1])+\":TriesEarlyStopping=10\\n\"\n",
    "            params.write(string1)\n",
    "            string2 = \"nTrain_Signal=\"+str(cp[2])+\"%:nTrain_Background=\"+str(cp[3])+\"%:SplitMode=Random:NormMode=NumEvents:!V\\n\"\n",
    "            training.write(string2)\n",
    "            string3 = str(cp[4])+','+str(cp[5])+','+str(cp[6])+','+str(cp[7])+','+str(cp[8])+','+str(cp[9])+','+str(cp[10])+'\\n'\n",
    "            model.write(string3)\n",
    "            string4 = str(cp[11])+','+str(cp[12])+'\\n'\n",
    "            comp_p.write(string4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_params=\"dnn_params2.txt\"\n",
    "file_training=\"dnn_training2.txt\"\n",
    "file_model=\"dnn_model2.csv\"\n",
    "file_comp_params='comp_params.txt'\n",
    "model_name=\"model_dense.h5\"\n",
    "arr_NumEpochs=[10]\n",
    "arr_BatchSize=[100, 200]\n",
    "arr_nTrain_Signal=[80]\n",
    "arr_nTrain_Background=[80]\n",
    "arr_number_HL=[3]\n",
    "arr_neurons=[64]\n",
    "arr_neurons_LF=[2]\n",
    "arr_k_init=['glorot_normal']\n",
    "arr_activation_IL=['sigmoid']\n",
    "arr_activation_HL=['relu']\n",
    "arr_activation_FL=['softmax']\n",
    "arr_loss=['categorical_crossentropy']\n",
    "arr_metrics=['categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_combs_params(file_params, file_training, file_model, file_comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_opt(config, params, training, model, comp_params, model_name):\n",
    "    max_roc = 0\n",
    "    best_params = \"\"\n",
    "    best_train = \"\"\n",
    "    best_model = \"\"\n",
    "    print(config)\n",
    "    print(\"===============\")\n",
    "    for i in range(len(params)):\n",
    "        roc = DNN(params[i], training[i], model[i], comp_params[i], model_name, config)\n",
    "        if roc > max_roc:\n",
    "            max_roc = roc\n",
    "            best_params = params[i]\n",
    "            best_train = training[i]\n",
    "            best_model = model[i]\n",
    "    best_model = best_model.split(',')\n",
    "    best_model_str = \"numero_HL=\"+str(best_model[0])+\", neurons=\"+str(best_model[1])+\", neurons_LF=\"+str(best_model[2])+\", k_init=\"+str(best_model[3])+\", activation_IL=\"+str(best_model[4])+\", activation_HL=\"+str(best_model[5])+\", activation_FL=\"+str(best_model[6])\n",
    "    print(\"best parameters:\", best_params)\n",
    "    print(\"best training:\", best_train)\n",
    "    print(\"best model:\", best_model_str)\n",
    "    print(\"ROC integral:\", max_roc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"dnn_params2.txt\"\n",
    "params = get_params(params_path)\n",
    "training_path = \"dnn_training2.txt\"\n",
    "training = get_params(training_path)\n",
    "configs = [\"PreSel_0tag_Xtohh1000\", \"PreSel_1tag_Xtohh1000\", \"PreSel_2tag_Xtohh1000\", \n",
    "           \"QCDCR_0tag_Xtohh1000\", \"QCDCR_1tag_Xtohh1000\", \"QCDCR_2tag_Xtohh1000\",\n",
    "           \"SR_0tag_Xtohh1000\", \"SR_1tag_Xtohh1000\", \"SR_2tag_Xtohh1000\",\n",
    "           \"PreSel_0tag_Xtohh2000\", \"PreSel_1tag_Xtohh2000\", \"PreSel_2tag_Xtohh2000\",\n",
    "           \"QCDCR_0tag_Xtohh2000\", \"QCDCR_1tag_Xtohh2000\", \"QCDCR_2tag_Xtohh2000\",\n",
    "           \"SR_0tag_Xtohh2000\", \"SR_1tag_Xtohh2000\", \"SR_2tag_Xtohh2000\"]\n",
    "s_end = \"_s.root\"\n",
    "b_end = \"_b.root\"\n",
    "comp_params_path = \"comp_params.txt\"\n",
    "comp_params = get_params(comp_params_path)\n",
    "model_input_path = \"dnn_model2.csv\"\n",
    "model_input = get_params(model_input_path)\n",
    "model_name = \"model_dense.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreSel_2tag_Xtohh1000\n",
      "===============\n",
      "<class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 160 samples, validate on 5178 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6917 - categorical_accuracy: 0.5188 - val_loss: 0.1218 - val_categorical_accuracy: 0.6288\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12177, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 668us/step - loss: 0.6876 - categorical_accuracy: 0.6437 - val_loss: 0.1237 - val_categorical_accuracy: 0.8384\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12177\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 631us/step - loss: 0.6864 - categorical_accuracy: 0.6125 - val_loss: 0.1220 - val_categorical_accuracy: 0.8067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12177\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 613us/step - loss: 0.6833 - categorical_accuracy: 0.6312 - val_loss: 0.1129 - val_categorical_accuracy: 0.1904\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12177 to 0.11294, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 624us/step - loss: 0.6816 - categorical_accuracy: 0.5313 - val_loss: 0.1147 - val_categorical_accuracy: 0.2480\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11294\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 621us/step - loss: 0.6785 - categorical_accuracy: 0.5750 - val_loss: 0.1194 - val_categorical_accuracy: 0.7086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11294\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 656us/step - loss: 0.6757 - categorical_accuracy: 0.6938 - val_loss: 0.1240 - val_categorical_accuracy: 0.8789\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11294\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 640us/step - loss: 0.6746 - categorical_accuracy: 0.6250 - val_loss: 0.1279 - val_categorical_accuracy: 0.8621\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11294\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 675us/step - loss: 0.6705 - categorical_accuracy: 0.6313 - val_loss: 0.1186 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11294\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 571us/step - loss: 0.6668 - categorical_accuracy: 0.7000 - val_loss: 0.1117 - val_categorical_accuracy: 0.4112\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11294 to 0.11169, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "ROC integral: 0.8203347113320818\n",
      "<class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 160 samples, validate on 5178 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6915 - categorical_accuracy: 0.6062 - val_loss: 0.1190 - val_categorical_accuracy: 0.1796\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11904, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 414us/step - loss: 0.6892 - categorical_accuracy: 0.4938 - val_loss: 0.1197 - val_categorical_accuracy: 0.2223\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.11904\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 370us/step - loss: 0.6882 - categorical_accuracy: 0.5437 - val_loss: 0.1225 - val_categorical_accuracy: 0.7082\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11904\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 332us/step - loss: 0.6871 - categorical_accuracy: 0.7250 - val_loss: 0.1243 - val_categorical_accuracy: 0.8694\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11904\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 384us/step - loss: 0.6861 - categorical_accuracy: 0.6750 - val_loss: 0.1245 - val_categorical_accuracy: 0.8694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11904\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 348us/step - loss: 0.6850 - categorical_accuracy: 0.6687 - val_loss: 0.1233 - val_categorical_accuracy: 0.8598\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11904\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 363us/step - loss: 0.6839 - categorical_accuracy: 0.7188 - val_loss: 0.1218 - val_categorical_accuracy: 0.6698\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11904\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 390us/step - loss: 0.6827 - categorical_accuracy: 0.7250 - val_loss: 0.1205 - val_categorical_accuracy: 0.4861\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11904\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 386us/step - loss: 0.6814 - categorical_accuracy: 0.6313 - val_loss: 0.1202 - val_categorical_accuracy: 0.4820\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11904\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 339us/step - loss: 0.6799 - categorical_accuracy: 0.6313 - val_loss: 0.1204 - val_categorical_accuracy: 0.5527\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11904\n",
      "ROC integral: 0.76485230262862\n",
      "best parameters: H:!V:VarTransform=N_AllClasses:FilenameModel=model_dense.h5:NumEpochs=10:BatchSize=100:TriesEarlyStopping=10\n",
      "best training: nTrain_Signal=80%:nTrain_Background=80%:SplitMode=Random:NormMode=NumEvents:!V\n",
      "best model: numero_HL=3, neurons=64, neurons_LF=2, k_init=glorot_normal, activation_IL=sigmoid, activation_HL=relu, activation_FL=softmax\n",
      "ROC integral: 0.8203347113320818\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.820\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.003 (0.033)       0.373 (0.245)      0.806 (0.675)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.765\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.001 (0.000)       0.239 (0.250)      0.672 (0.775)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n"
     ]
    }
   ],
   "source": [
    "param_opt(configs[2], params, training, model_input, comp_params, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
