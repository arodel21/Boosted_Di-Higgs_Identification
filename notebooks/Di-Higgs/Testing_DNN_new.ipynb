{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TMVA Classification Using Deep Neural Networks</center>\n",
    "\n",
    "In this notebook we still classify di-Higgs new data with Deep Neural Networks meethod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA, TTree\n",
    "import pandas as pd\n",
    "\n",
    "ROOT.TMVA.Tools.Instance()\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize()\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by region.\n",
    "\n",
    "This function will let you filter your dataset by region. It's known that SR_1tag is very signal poor, while SR_2tag has a lot a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_region(file, region, signal):\n",
    "    oldfile = ROOT.TFile(file)\n",
    "    oldtree = oldfile.Nominal\n",
    "    signal_file = ROOT.TFile(region+\"_\"+signal+\"_s.root\",\"recreate\")\n",
    "    signal_tree = oldtree.CloneTree(0)\n",
    "    backg_file = ROOT.TFile(region+\"_\"+signal+\"_b.root\",\"recreate\")\n",
    "    backg_tree = oldtree.CloneTree(0)\n",
    "    data_file = ROOT.TFile(region+\"_\"+signal+\"_d.root\",\"recreate\")\n",
    "    data_tree = oldtree.CloneTree(0)\n",
    "    for entry in oldtree:\n",
    "        if (entry.m_region == region):\n",
    "            if (entry.sample == \"data\"):\n",
    "                data_tree.Fill()\n",
    "            elif (entry.sample == \"Xtohh1000\"): #signal\n",
    "                signal_tree.Fill()\n",
    "            else:\n",
    "                backg_tree.Fill()\n",
    "    signal_tree.AutoSave()   \n",
    "    backg_tree.AutoSave()\n",
    "    data_tree.AutoSave()\n",
    "    return signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file\n",
    "\n",
    "#Use as\n",
    "#tree, file = filter_region(\"data.root\", \"SR_1tag\", \"small.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory and Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.root has unlabeled data points (called data) and fakes points. For the background training we'll use only the fakes points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldtree, oldfile = filter_region(\"data.root\", \"SR_2tag\", \"data_f.root\")\n",
    "\n",
    "newfile = ROOT.TFile(\"small.root\",\"recreate\")\n",
    "backgroundTreeB1 = oldtree.CloneTree(0)\n",
    "\n",
    "for entry in oldtree:\n",
    "    sample = entry.sample\n",
    "    reg = entry.m_region\n",
    "    if (sample == 'fakes'):\n",
    "        backgroundTreeB1.Fill()\n",
    "\n",
    "backgroundTreeB1.AutoSave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************\n",
      "*Tree    :Nominal   : Nominal                                                *\n",
      "*Entries :      127 : Total =           25070 bytes  File  Size =      10560 *\n",
      "*        :          : Tree compression factor =   1.00                       *\n",
      "******************************************************************************\n",
      "*Br    0 :sample    : string                                                 *\n",
      "*Entries :      127 : Total  Size=       1935 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    1 :EventWeight : EventWeight/F                                        *\n",
      "*Entries :      127 : Total  Size=       1195 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    2 :EventNumber : EventNumber/l                                        *\n",
      "*Entries :      127 : Total  Size=       1711 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    3 :m_region  : string                                                 *\n",
      "*Entries :      127 : Total  Size=       2201 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    4 :m_FJNbtagJets : m_FJNbtagJets/I                                    *\n",
      "*Entries :      127 : Total  Size=       1207 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    5 :m_FJpt    : m_FJpt/F                                               *\n",
      "*Entries :      127 : Total  Size=       1165 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    6 :m_FJeta   : m_FJeta/F                                              *\n",
      "*Entries :      127 : Total  Size=       1171 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    7 :m_FJphi   : m_FJphi/F                                              *\n",
      "*Entries :      127 : Total  Size=       1171 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    8 :m_FJm     : m_FJm/F                                                *\n",
      "*Entries :      127 : Total  Size=       1159 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    9 :m_DTpt    : m_DTpt/F                                               *\n",
      "*Entries :      127 : Total  Size=       1165 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   10 :m_DTeta   : m_DTeta/F                                              *\n",
      "*Entries :      127 : Total  Size=       1171 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   11 :m_DTphi   : m_DTphi/F                                              *\n",
      "*Entries :      127 : Total  Size=       1171 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   12 :m_DTm     : m_DTm/F                                                *\n",
      "*Entries :      127 : Total  Size=       1159 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   13 :m_dPhiFTwDT : m_dPhiFTwDT/F                                        *\n",
      "*Entries :      127 : Total  Size=       1195 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   14 :m_dRFJwDT : m_dRFJwDT/F                                            *\n",
      "*Entries :      127 : Total  Size=       1183 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   15 :m_dPhiDTwMET : m_dPhiDTwMET/F                                      *\n",
      "*Entries :      127 : Total  Size=       1201 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   16 :m_MET     : m_MET/F                                                *\n",
      "*Entries :      127 : Total  Size=       1159 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   17 :m_hhm     : m_hhm/F                                                *\n",
      "*Entries :      127 : Total  Size=       1159 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   18 :m_bbttpt  : m_bbttpt/F                                             *\n",
      "*Entries :      127 : Total  Size=       1177 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n"
     ]
    }
   ],
   "source": [
    "outputFile = ROOT.TFile.Open(\"Higgs_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "# Factory\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "# Input data\n",
    "# We define now the input data file and we retrieve the ROOT TTree objects with the signal and background input events\n",
    "#signalTree1, inputFileS1 = filter_region(\"Xtohh1000.root\", \"SR_1tag\", \"Xtohh1000_f.root\")\n",
    "signalTree2, inputFileS2 = filter_region(\"Xtohh2000.root\", \"SR_2tag\", \"Xtohh2000_f.root\")\n",
    "backgroundTreeB2, inputFileB2 = filter_region(\"stop.root\", \"SR_2tag\", \"stop_f.root\")\n",
    "backgroundTreeB3, inputFileB3 = filter_region(\"ttbar.root\", \"SR_2tag\", \"ttbar_f.root\")\n",
    "backgroundTreeB4, inputFileB4 = filter_region(\"W+jets.root\", \"SR_2tag\", \"W+jets_f.root\")\n",
    "backgroundTreeB5, inputFileB5 = filter_region(\"Zee_221.root\", \"SR_2tag\", \"Zee_221_f.root\")\n",
    "backgroundTreeB6, inputFileB6 = filter_region(\"Ztautau_221.root\", \"SR_2tag\", \"Ztautau_221_f.root\")\n",
    "backgroundTreeB7, inputFileB7 = filter_region(\"ZZ_Pw.root\", \"SR_2tag\", \"ZZ_Pw_f.root\")\n",
    "\n",
    "backgroundTreeB1.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input data abd variables \n",
    "\n",
    "We add first the signal and background trees in the data loader and then we\n",
    "define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "We have two kinds of signals and for the training we have to use only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree Nominal of type Signal with 11449 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree Nominal of type Background with 127 events\n",
      "                         : Add Tree Nominal of type Background with 8 events\n",
      "                         : Add Tree Nominal of type Background with 52 events\n",
      "                         : Add Tree Nominal of type Background with 69 events\n",
      "                         : Add Tree Nominal of type Background with 10 events\n",
      "                         : Add Tree Nominal of type Background with 164 events\n",
      "                         : Add Tree Nominal of type Background with 9 events\n"
     ]
    }
   ],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "#loader.AddSignalTree    ( signalTree1,     signalWeight     )\n",
    "loader.AddSignalTree    ( signalTree2,     signalWeight     )\n",
    "loader.AddBackgroundTree( backgroundTreeB1, backgroundWeight )\n",
    "loader.AddBackgroundTree( backgroundTreeB2, backgroundWeight )\n",
    "loader.AddBackgroundTree( backgroundTreeB3, backgroundWeight )\n",
    "loader.AddBackgroundTree( backgroundTreeB4, backgroundWeight )\n",
    "loader.AddBackgroundTree( backgroundTreeB5, backgroundWeight )\n",
    "loader.AddBackgroundTree( backgroundTreeB6, backgroundWeight )\n",
    "loader.AddBackgroundTree( backgroundTreeB7, backgroundWeight )\n",
    "\n",
    "not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "## Define input variables \n",
    "for branch in backgroundTreeB1.GetListOfBranches():\n",
    "    if branch.GetName() in not_cons:\n",
    "        continue\n",
    "    loader.AddVariable(branch.GetName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Setup the DataLoader by splitting events in training and test samples. \n",
    "Here we use a random split and a fixed number of training and test events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "loader.PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=8000:nTrain_Background=400:SplitMode=Random:\"\n",
    "                                   \"NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='sigmoid', input_dim=10))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy',])\n",
    "\n",
    "# Store model to file\n",
    "model.save('model_dense.h5')\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ROOT.TMVA::MethodPyKeras object (\"Keras_Dense\") at 0xbecc5f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mKeras_Dense\u001b[0m\n",
      "                         : \n",
      "Keras_Dense              : [dataset] : Create Transformation \"G\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'm_FJpt' <---> Output : variable 'm_FJpt'\n",
      "                         : Input : variable 'm_FJm' <---> Output : variable 'm_FJm'\n",
      "                         : Input : variable 'm_DTpt' <---> Output : variable 'm_DTpt'\n",
      "                         : Input : variable 'm_DTm' <---> Output : variable 'm_DTm'\n",
      "                         : Input : variable 'm_dPhiFTwDT' <---> Output : variable 'm_dPhiFTwDT'\n",
      "                         : Input : variable 'm_dRFJwDT' <---> Output : variable 'm_dRFJwDT'\n",
      "                         : Input : variable 'm_dPhiDTwMET' <---> Output : variable 'm_dPhiDTwMET'\n",
      "                         : Input : variable 'm_MET' <---> Output : variable 'm_MET'\n",
      "                         : Input : variable 'm_hhm' <---> Output : variable 'm_hhm'\n",
      "                         : Input : variable 'm_bbttpt' <---> Output : variable 'm_bbttpt'\n",
      "                         : Load model from file: model_dense.h5\n"
     ]
    }
   ],
   "source": [
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, 'Keras_Dense',\n",
    "        'H:!V:VarTransform=G:FilenameModel=model_dense.h5:'+\\\n",
    "        'NumEpochs=10:BatchSize=16:TriesEarlyStopping=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/lib/python3.5/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8400 samples, validate on 3488 samples\n",
      "Epoch 1/10\n",
      "8400/8400 [==============================] - 2s 253us/step - loss: 0.1254 - categorical_accuracy: 0.9658 - val_loss: 0.0401 - val_categorical_accuracy: 0.9908\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04009, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "8400/8400 [==============================] - 2s 212us/step - loss: 0.0928 - categorical_accuracy: 0.9744 - val_loss: 0.0349 - val_categorical_accuracy: 0.9903\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04009 to 0.03493, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/10\n",
      "8400/8400 [==============================] - 2s 218us/step - loss: 0.0861 - categorical_accuracy: 0.9758 - val_loss: 0.0892 - val_categorical_accuracy: 0.9722\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03493\n",
      "Epoch 4/10\n",
      "8400/8400 [==============================] - 2s 262us/step - loss: 0.0864 - categorical_accuracy: 0.9774 - val_loss: 0.0589 - val_categorical_accuracy: 0.9857\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03493\n",
      "Epoch 5/10\n",
      "8400/8400 [==============================] - 3s 301us/step - loss: 0.0819 - categorical_accuracy: 0.9761 - val_loss: 0.0410 - val_categorical_accuracy: 0.9888\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03493\n",
      "Epoch 6/10\n",
      "8400/8400 [==============================] - 2s 297us/step - loss: 0.0824 - categorical_accuracy: 0.9764 - val_loss: 0.0349 - val_categorical_accuracy: 0.9908\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03493\n",
      "Epoch 7/10\n",
      "8400/8400 [==============================] - 2s 296us/step - loss: 0.0804 - categorical_accuracy: 0.9769 - val_loss: 0.0645 - val_categorical_accuracy: 0.9768\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03493\n",
      "Epoch 8/10\n",
      "8400/8400 [==============================] - 3s 300us/step - loss: 0.0810 - categorical_accuracy: 0.9760 - val_loss: 0.0298 - val_categorical_accuracy: 0.9905\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03493 to 0.02981, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/10\n",
      "8400/8400 [==============================] - 2s 270us/step - loss: 0.0813 - categorical_accuracy: 0.9762 - val_loss: 0.0352 - val_categorical_accuracy: 0.9888\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02981\n",
      "Epoch 10/10\n",
      "8400/8400 [==============================] - 2s 262us/step - loss: 0.0813 - categorical_accuracy: 0.9767 - val_loss: 0.0258 - val_categorical_accuracy: 0.9928\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02981 to 0.02583, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 8000\n",
      "                         : Signal     -- testing events             : 3449\n",
      "                         : Signal     -- training and testing events: 11449\n",
      "                         : Background -- training events            : 400\n",
      "                         : Background -- testing events             : 39\n",
      "                         : Background -- training and testing events: 439\n",
      "                         : \n",
      "DataSetInfo              : Correlation matrix (Signal):\n",
      "                         : ---------------------------------------------------------------------------------------------------------\n",
      "                         :                m_FJpt   m_FJm  m_DTpt   m_DTm m_dPhiFTwDT m_dRFJwDT m_dPhiDTwMET   m_MET   m_hhm m_bbttpt\n",
      "                         :       m_FJpt:  +1.000  +0.228  +0.286  +0.129      +0.007    -0.592       -0.005  +0.316  +0.170   +0.510\n",
      "                         :        m_FJm:  +0.228  +1.000  +0.096  +0.046      +0.035    +0.035       -0.013  +0.084  +0.298   +0.084\n",
      "                         :       m_DTpt:  +0.286  +0.096  +1.000  +0.623      +0.044    -0.294       -0.020  -0.439  +0.703   -0.383\n",
      "                         :        m_DTm:  +0.129  +0.046  +0.623  +1.000      +0.025    -0.074       -0.011  -0.306  +0.582   -0.291\n",
      "                         :  m_dPhiFTwDT:  +0.007  +0.035  +0.044  +0.025      +1.000    +0.550       +0.009  -0.048  +0.021   -0.557\n",
      "                         :    m_dRFJwDT:  -0.592  +0.035  -0.294  -0.074      +0.550    +1.000       +0.016  -0.296  +0.188   -0.561\n",
      "                         : m_dPhiDTwMET:  -0.005  -0.013  -0.020  -0.011      +0.009    +0.016       +1.000  +0.006  -0.008   +0.002\n",
      "                         :        m_MET:  +0.316  +0.084  -0.439  -0.306      -0.048    -0.296       +0.006  +1.000  -0.529   +0.553\n",
      "                         :        m_hhm:  +0.170  +0.298  +0.703  +0.582      +0.021    +0.188       -0.008  -0.529  +1.000   -0.341\n",
      "                         :     m_bbttpt:  +0.510  +0.084  -0.383  -0.291      -0.557    -0.561       +0.002  +0.553  -0.341   +1.000\n",
      "                         : ---------------------------------------------------------------------------------------------------------\n",
      "DataSetInfo              : Correlation matrix (Background):\n",
      "                         : ---------------------------------------------------------------------------------------------------------\n",
      "                         :                m_FJpt   m_FJm  m_DTpt   m_DTm m_dPhiFTwDT m_dRFJwDT m_dPhiDTwMET   m_MET   m_hhm m_bbttpt\n",
      "                         :       m_FJpt:  +1.000  +0.318  +0.387  +0.061      +0.063    +0.012       +0.039  +0.623  +0.567   +0.499\n",
      "                         :        m_FJm:  +0.318  +1.000  +0.122  -0.015      -0.063    -0.117       +0.027  +0.217  +0.144   +0.192\n",
      "                         :       m_DTpt:  +0.387  +0.122  +1.000  +0.548      +0.193    +0.146       -0.025  -0.062  +0.572   -0.126\n",
      "                         :        m_DTm:  +0.061  -0.015  +0.548  +1.000      +0.118    +0.129       -0.073  -0.117  +0.296   -0.150\n",
      "                         :  m_dPhiFTwDT:  +0.063  -0.063  +0.193  +0.118      +1.000    +0.656       -0.037  -0.092  +0.162   -0.685\n",
      "                         :    m_dRFJwDT:  +0.012  -0.117  +0.146  +0.129      +0.656    +1.000       -0.016  -0.124  +0.623   -0.496\n",
      "                         : m_dPhiDTwMET:  +0.039  +0.027  -0.025  -0.073      -0.037    -0.016       +1.000  +0.035  +0.008   +0.055\n",
      "                         :        m_MET:  +0.623  +0.217  -0.062  -0.117      -0.092    -0.124       +0.035  +1.000  +0.174   +0.555\n",
      "                         :        m_hhm:  +0.567  +0.144  +0.572  +0.296      +0.162    +0.623       +0.008  +0.174  +1.000   +0.078\n",
      "                         :     m_bbttpt:  +0.499  +0.192  -0.126  -0.150      -0.685    -0.496       +0.055  +0.555  +0.078   +1.000\n",
      "                         : ---------------------------------------------------------------------------------------------------------\n",
      "DataSetFactory           : [dataset] :  \n",
      "                         : \n",
      "Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'm_FJpt' <---> Output : variable 'm_FJpt'\n",
      "                         : Input : variable 'm_FJm' <---> Output : variable 'm_FJm'\n",
      "                         : Input : variable 'm_DTpt' <---> Output : variable 'm_DTpt'\n",
      "                         : Input : variable 'm_DTm' <---> Output : variable 'm_DTm'\n",
      "                         : Input : variable 'm_dPhiFTwDT' <---> Output : variable 'm_dPhiFTwDT'\n",
      "                         : Input : variable 'm_dRFJwDT' <---> Output : variable 'm_dRFJwDT'\n",
      "                         : Input : variable 'm_dPhiDTwMET' <---> Output : variable 'm_dPhiDTwMET'\n",
      "                         : Input : variable 'm_MET' <---> Output : variable 'm_MET'\n",
      "                         : Input : variable 'm_hhm' <---> Output : variable 'm_hhm'\n",
      "                         : Input : variable 'm_bbttpt' <---> Output : variable 'm_bbttpt'\n",
      "TFHandler_Factory        :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :       m_FJpt:        820.21        165.67   [        257.18        1776.3 ]\n",
      "                         :        m_FJm:    1.2298e+05        51647.   [        0.0000    7.7804e+05 ]\n",
      "                         :       m_DTpt:        599.99        157.77   [        307.74        1177.9 ]\n",
      "                         :        m_DTm:    1.5456e+05        42164.   [        45911.    4.0037e+05 ]\n",
      "                         :  m_dPhiFTwDT:        2.9897       0.21218   [       0.13455        3.1416 ]\n",
      "                         :    m_dRFJwDT:        3.1890       0.27239   [        1.3826        4.3342 ]\n",
      "                         : m_dPhiDTwMET:    -0.0033038       0.25078   [       -1.0066        1.0202 ]\n",
      "                         :        m_MET:        271.81        162.26   [        10.707        1076.8 ]\n",
      "                         :        m_hhm:        1611.3        245.74   [        628.11        3737.6 ]\n",
      "                         :     m_bbttpt:        284.88        180.08   [        1.8624        1867.7 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         : Ranking input variables (method unspecific)...\n",
      "IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : -------------------------------------\n",
      "                         : Rank : Variable     : Separation\n",
      "                         : -------------------------------------\n",
      "                         :    1 : m_FJpt       : 4.686e-01\n",
      "                         :    2 : m_hhm        : 4.602e-01\n",
      "                         :    3 : m_FJm        : 3.340e-01\n",
      "                         :    4 : m_MET        : 2.078e-01\n",
      "                         :    5 : m_DTpt       : 1.570e-01\n",
      "                         :    6 : m_bbttpt     : 1.547e-01\n",
      "                         :    7 : m_dPhiDTwMET : 1.142e-01\n",
      "                         :    8 : m_dRFJwDT    : 3.791e-02\n",
      "                         :    9 : m_DTm        : 2.911e-02\n",
      "                         :   10 : m_dPhiFTwDT  : 2.491e-02\n",
      "                         : -------------------------------------\n",
      "Factory                  : Train method: Keras_Dense for Classification\n",
      "                         : \n",
      "                         : \n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \u001b[1mH e l p   f o r   M V A   m e t h o d   [ Keras_Dense ] :\u001b[0m\n",
      "                         : \n",
      "                         : Keras is a high-level API for the Theano and Tensorflow packages.\n",
      "                         : This method wraps the training and predictions steps of the Keras\n",
      "                         : Python package for TMVA, so that dataloading, preprocessing and\n",
      "                         : evaluation can be done within the TMVA system. To use this Keras\n",
      "                         : interface, you have to generate a model with Keras first. Then,\n",
      "                         : this model can be loaded and trained in TMVA.\n",
      "                         : \n",
      "                         : \n",
      "                         : <Suppress this message by specifying \"!H\" in the booking option>\n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \n",
      "                         : Preparing the Gaussian transformation...\n",
      "TFHandler_Keras_Dense    :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :       m_FJpt:     0.0075764        1.0005   [       -3.1339        5.7307 ]\n",
      "                         :        m_FJm:     0.0072939       0.99933   [       -3.1339        5.7307 ]\n",
      "                         :       m_DTpt:     0.0074324       0.99968   [       -3.1339        5.7307 ]\n",
      "                         :        m_DTm:     0.0064531        1.0030   [       -3.2712        5.7307 ]\n",
      "                         :  m_dPhiFTwDT:     0.0084848        1.0037   [       -3.1339        5.7307 ]\n",
      "                         :    m_dRFJwDT:     0.0075341       0.99951   [       -3.1338        5.7307 ]\n",
      "                         : m_dPhiDTwMET:     0.0076834        1.0011   [       -3.1327        5.7307 ]\n",
      "                         :        m_MET:     0.0071858       0.99963   [       -3.1339        5.7307 ]\n",
      "                         :        m_hhm:     0.0069572       0.99750   [       -3.1339        5.7307 ]\n",
      "                         :     m_bbttpt:     0.0076580        1.0005   [       -3.1339        5.7307 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored\n",
      "                         : Option TriesEarlyStopping: Training will stop after 10 number of epochs with no improvement of validation loss\n",
      "                         : Elapsed time for training with 8400 events: 23.8 sec         \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_Higgs_Classification_Keras_Dense.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_Higgs_Classification_Keras_Dense.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "                         : No variable ranking supplied by classifier: Keras_Dense\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_Higgs_Classification_Keras_Dense.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Model\n",
    "\n",
    "Here we test all methods using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: Keras_Dense for Classification performance\n",
      "                         : \n",
      "                         : Load model from file: dataset/weights/TrainedModel_Keras_Dense.h5\n"
     ]
    }
   ],
   "source": [
    "factory.TestAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Here we evaluate all methods and compare their performances, computing efficiencies, ROC curves etc.. using both training and tetsing data sets. Several histograms are produced which can be examined with the TMVAGui or directly using the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: Keras_Dense\n",
      "                         : \n",
      "TFHandler_Keras_Dense    :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :       m_FJpt:      0.061793       0.98403   [       -2.8189        5.7307 ]\n",
      "                         :        m_FJm:     0.0033283       0.97446   [       -2.8114        5.7307 ]\n",
      "                         :       m_DTpt:      0.028396       0.97060   [       -3.0425        5.7307 ]\n",
      "                         :        m_DTm:      0.021749       0.97155   [       -2.9308        5.7307 ]\n",
      "                         :  m_dPhiFTwDT:      0.022585        1.0070   [       -3.1322        5.7307 ]\n",
      "                         :    m_dRFJwDT:      0.014520       0.98403   [       -3.7850        5.7307 ]\n",
      "                         : m_dPhiDTwMET:     -0.021543       0.98067   [       -5.7307        5.7307 ]\n",
      "                         :        m_MET:      0.015343        1.0042   [       -3.0496        5.7307 ]\n",
      "                         :        m_hhm:      0.051457       0.92772   [       -2.8525        3.4831 ]\n",
      "                         :     m_bbttpt:      0.025240       0.97304   [       -3.0152        5.7307 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "Keras_Dense              : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "TFHandler_Keras_Dense    :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :       m_FJpt:      0.061793       0.98403   [       -2.8189        5.7307 ]\n",
      "                         :        m_FJm:     0.0033283       0.97446   [       -2.8114        5.7307 ]\n",
      "                         :       m_DTpt:      0.028396       0.97060   [       -3.0425        5.7307 ]\n",
      "                         :        m_DTm:      0.021749       0.97155   [       -2.9308        5.7307 ]\n",
      "                         :  m_dPhiFTwDT:      0.022585        1.0070   [       -3.1322        5.7307 ]\n",
      "                         :    m_dRFJwDT:      0.014520       0.98403   [       -3.7850        5.7307 ]\n",
      "                         : m_dPhiDTwMET:     -0.021543       0.98067   [       -5.7307        5.7307 ]\n",
      "                         :        m_MET:      0.015343        1.0042   [       -3.0496        5.7307 ]\n",
      "                         :        m_hhm:      0.051457       0.92772   [       -2.8525        3.4831 ]\n",
      "                         :     m_bbttpt:      0.025240       0.97304   [       -3.0152        5.7307 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.965\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.551 (0.238)       0.854 (0.851)      0.994 (0.987)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TestTree' with 3488 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 8400 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(file):\n",
    "    params = []\n",
    "    first = 1\n",
    "    with open(file, 'r') as fp:\n",
    "        line = fp.readline().rstrip()\n",
    "        while line:\n",
    "            if (file.split('.')[1] == 'csv' and first):\n",
    "                first = 0\n",
    "                line = fp.readline().rstrip()\n",
    "                continue\n",
    "            params.append(line)\n",
    "            line = fp.readline().rstrip()       \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(params, training, model_input, comp_params, model_name, config):\n",
    "    \n",
    "    output_file = config+\"_DNN_Classification.root\"\n",
    "    signal_file = config+\"_s.root\"\n",
    "    backg_file = config+\"_b.root\"\n",
    "    \n",
    "    signal_input = ROOT.TFile(signal_file)\n",
    "    signal_tree = signal_input.Nominal\n",
    "    \n",
    "    backg_input = ROOT.TFile(backg_file)\n",
    "    backg_tree = backg_input.Nominal\n",
    "    \n",
    "    outputFile = ROOT.TFile.Open(output_file, \"RECREATE\")\n",
    "\n",
    "    # Factory\n",
    "    factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                          \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "    loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "    ### global event weights per tree (see below for setting event-wise weights)\n",
    "    signalWeight     = 1.0\n",
    "    backgroundWeight = 1.0\n",
    "\n",
    "    ### You can add an arbitrary number of signal or background trees\n",
    "    loader.AddSignalTree    ( signal_tree,     signalWeight     )\n",
    "    loader.AddBackgroundTree( backg_tree, backgroundWeight )\n",
    "\n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "    ## Define input variables \n",
    "    for branch in backg_tree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        loader.AddVariable(branch.GetName())\n",
    "        \n",
    "    mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "    mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "    loader.PrepareTrainingAndTestTree(mycuts, mycutb, training)\n",
    "    \n",
    "    # Model structure\n",
    "    \n",
    "    comp_params = comp_params.rstrip()\n",
    "    comp_params = comp_params.split(',')\n",
    "    loss = comp_params[0]\n",
    "    \n",
    "    comp_params.remove(loss)\n",
    "    metrics = comp_params\n",
    "    \n",
    "    model = Sequential()\n",
    "    model_input = model_input.rstrip()\n",
    "    model_input = model_input.split(',')\n",
    "    \n",
    "    hidden_l = int(model_input[0])\n",
    "    neurons = int(model_input[1])\n",
    "    neurons_LF = int(model_input[2])\n",
    "    k_init = model_input[3]\n",
    "    activation_IL = model_input[4]\n",
    "    activation_HL = model_input[5]\n",
    "    activation_FL = model_input[6]\n",
    "    \n",
    "    print(type(neurons))\n",
    "    \n",
    "    model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_IL, input_dim=10))\n",
    "    for h in range(hidden_l):\n",
    "        model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_HL))\n",
    "        \n",
    "    model.add(Dense(neurons_LF, kernel_initializer=k_init, activation=activation_FL))\n",
    "    \n",
    "    # Set loss and optimizer\n",
    "    model.compile(loss=loss, optimizer=Adam(), metrics=metrics)\n",
    "    # Store model to file\n",
    "    model.save(model_name)\n",
    "    # Print summary of model\n",
    "    model.summary()\n",
    "    \n",
    "    ## DNN method\n",
    "    factory.BookMethod(loader,ROOT.TMVA.Types.kPyKeras, \"Keras_Dense\", params)\n",
    "        \n",
    "    factory.TrainAllMethods()\n",
    "    \n",
    "    factory.TestAllMethods()\n",
    "    \n",
    "    factory.EvaluateAllMethods()\n",
    "    \n",
    "    c1 = factory.GetROCCurve(loader)\n",
    "    #c1.Draw()\n",
    "    \n",
    "    integ = factory.GetROCIntegral(loader, \"Keras_Dense\")\n",
    "    \n",
    "    print(\"ROC integral:\", integ)\n",
    "    \n",
    "    outputFile.Close()\n",
    "    signal_input.Close()\n",
    "    backg_input.Close()\n",
    "    \n",
    "    return integ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_combs_params(file_params, file_training, file_model, comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics):\n",
    "    comb_params = list(itertools.product(arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics))\n",
    "    with open(file_params, 'w') as params, open(file_training, 'w') as training, open(file_model, 'w') as model, open(comp_params, 'w') as comp_p:\n",
    "        model.write(\"number_HL,neurons,neurons_LF,k_init,activation_IL,activation_HL,activation_FL\\n\")\n",
    "        for cp in comb_params:\n",
    "            string1 = \"H:!V:VarTransform=N_AllClasses:FilenameModel=\"+model_name+\":NumEpochs=\"+str(cp[0])+\":BatchSize=\"+str(cp[1])+\":TriesEarlyStopping=10\\n\"\n",
    "            params.write(string1)\n",
    "            string2 = \"nTrain_Signal=\"+str(cp[2])+\"%:nTrain_Background=\"+str(cp[3])+\"%:SplitMode=Random:NormMode=NumEvents:!V\\n\"\n",
    "            training.write(string2)\n",
    "            string3 = str(cp[4])+','+str(cp[5])+','+str(cp[6])+','+str(cp[7])+','+str(cp[8])+','+str(cp[9])+','+str(cp[10])+'\\n'\n",
    "            model.write(string3)\n",
    "            string4 = str(cp[11])+','+str(cp[12])+'\\n'\n",
    "            comp_p.write(string4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_params=\"dnn_params2.txt\"\n",
    "file_training=\"dnn_training2.txt\"\n",
    "file_model=\"dnn_model2.csv\"\n",
    "file_comp_params='comp_params.txt'\n",
    "model_name=\"model_dense.h5\"\n",
    "arr_NumEpochs=[10]\n",
    "arr_BatchSize=[100, 200]\n",
    "arr_nTrain_Signal=[80]\n",
    "arr_nTrain_Background=[80]\n",
    "arr_number_HL=[3]\n",
    "arr_neurons=[64]\n",
    "arr_neurons_LF=[2]\n",
    "arr_k_init=['glorot_normal']\n",
    "arr_activation_IL=['sigmoid']\n",
    "arr_activation_HL=['relu']\n",
    "arr_activation_FL=['softmax']\n",
    "arr_loss=['categorical_crossentropy']\n",
    "arr_metrics=['categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_combs_params(file_params, file_training, file_model, file_comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_opt(config, params, training, model, comp_params, model_name):\n",
    "    max_roc = 0\n",
    "    best_params = \"\"\n",
    "    best_train = \"\"\n",
    "    best_model = \"\"\n",
    "    print(config)\n",
    "    print(\"===============\")\n",
    "    for i in range(len(params)):\n",
    "        roc = DNN(params[i], training[i], model[i], comp_params[i], model_name, config)\n",
    "        if roc > max_roc:\n",
    "            max_roc = roc\n",
    "            best_params = params[i]\n",
    "            best_train = training[i]\n",
    "            best_model = model[i]\n",
    "    best_model = best_model.split(',')\n",
    "    best_model_str = \"numero_HL=\"+str(best_model[0])+\", neurons=\"+str(best_model[1])+\", neurons_LF=\"+str(best_model[2])+\", k_init=\"+str(best_model[3])+\", activation_IL=\"+str(best_model[4])+\", activation_HL=\"+str(best_model[5])+\", activation_FL=\"+str(best_model[6])\n",
    "    print(\"best parameters:\", best_params)\n",
    "    print(\"best training:\", best_train)\n",
    "    print(\"best model:\", best_model_str)\n",
    "    print(\"ROC integral:\", max_roc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"dnn_params2.txt\"\n",
    "params = get_params(params_path)\n",
    "training_path = \"dnn_training2.txt\"\n",
    "training = get_params(training_path)\n",
    "configs = [\"PreSel_0tag_Xtohh1000\", \"PreSel_1tag_Xtohh1000\", \"PreSel_2tag_Xtohh1000\", \n",
    "           \"QCDCR_0tag_Xtohh1000\", \"QCDCR_1tag_Xtohh1000\", \"QCDCR_2tag_Xtohh1000\",\n",
    "           \"SR_0tag_Xtohh1000\", \"SR_1tag_Xtohh1000\", \"SR_2tag_Xtohh1000\",\n",
    "           \"PreSel_0tag_Xtohh2000\", \"PreSel_1tag_Xtohh2000\", \"PreSel_2tag_Xtohh2000\",\n",
    "           \"QCDCR_0tag_Xtohh2000\", \"QCDCR_1tag_Xtohh2000\", \"QCDCR_2tag_Xtohh2000\",\n",
    "           \"SR_0tag_Xtohh2000\", \"SR_1tag_Xtohh2000\", \"SR_2tag_Xtohh2000\"]\n",
    "s_end = \"_s.root\"\n",
    "b_end = \"_b.root\"\n",
    "comp_params_path = \"comp_params.txt\"\n",
    "comp_params = get_params(comp_params_path)\n",
    "model_input_path = \"dnn_model2.csv\"\n",
    "model_input = get_params(model_input_path)\n",
    "model_name = \"model_dense.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreSel_2tag_Xtohh1000\n",
      "===============\n",
      "2\n",
      "<class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 160 samples, validate on 5178 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.6980 - categorical_accuracy: 0.4875 - val_loss: 0.7191 - val_categorical_accuracy: 0.1765\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71915, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 714us/step - loss: 0.6939 - categorical_accuracy: 0.4937 - val_loss: 0.6961 - val_categorical_accuracy: 0.2677\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71915 to 0.69611, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 719us/step - loss: 0.6927 - categorical_accuracy: 0.4812 - val_loss: 0.6938 - val_categorical_accuracy: 0.3486\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69611 to 0.69383, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 690us/step - loss: 0.6887 - categorical_accuracy: 0.5938 - val_loss: 0.7162 - val_categorical_accuracy: 0.1937\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69383\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 687us/step - loss: 0.6900 - categorical_accuracy: 0.5000 - val_loss: 0.7162 - val_categorical_accuracy: 0.1989\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69383\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 729us/step - loss: 0.6870 - categorical_accuracy: 0.5375 - val_loss: 0.6848 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.69383 to 0.68481, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 662us/step - loss: 0.6849 - categorical_accuracy: 0.6563 - val_loss: 0.6629 - val_categorical_accuracy: 0.8413\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68481 to 0.66291, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 711us/step - loss: 0.6850 - categorical_accuracy: 0.5437 - val_loss: 0.6503 - val_categorical_accuracy: 0.8316\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.66291 to 0.65033, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 730us/step - loss: 0.6841 - categorical_accuracy: 0.5250 - val_loss: 0.6599 - val_categorical_accuracy: 0.8486\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.65033\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 689us/step - loss: 0.6819 - categorical_accuracy: 0.5937 - val_loss: 0.6781 - val_categorical_accuracy: 0.8229\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.65033\n",
      "ROC integral: 0.7847150919911989\n",
      "ROC 0 = 0.7847150919911989\n",
      "<class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 160 samples, validate on 5178 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.6930 - categorical_accuracy: 0.4938 - val_loss: 0.6723 - val_categorical_accuracy: 0.8235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67228, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 411us/step - loss: 0.6916 - categorical_accuracy: 0.5000 - val_loss: 0.6710 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67228 to 0.67099, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 397us/step - loss: 0.6896 - categorical_accuracy: 0.5063 - val_loss: 0.6760 - val_categorical_accuracy: 0.8438\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.67099\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 421us/step - loss: 0.6876 - categorical_accuracy: 0.5813 - val_loss: 0.6897 - val_categorical_accuracy: 0.5948\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67099\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 423us/step - loss: 0.6861 - categorical_accuracy: 0.6687 - val_loss: 0.7029 - val_categorical_accuracy: 0.2356\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.67099\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 377us/step - loss: 0.6849 - categorical_accuracy: 0.5750 - val_loss: 0.7016 - val_categorical_accuracy: 0.2511\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67099\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 376us/step - loss: 0.6837 - categorical_accuracy: 0.5813 - val_loss: 0.6902 - val_categorical_accuracy: 0.5116\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.67099\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 509us/step - loss: 0.6822 - categorical_accuracy: 0.6500 - val_loss: 0.6789 - val_categorical_accuracy: 0.8048\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.67099\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 387us/step - loss: 0.6809 - categorical_accuracy: 0.6938 - val_loss: 0.6745 - val_categorical_accuracy: 0.8372\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.67099\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 442us/step - loss: 0.6794 - categorical_accuracy: 0.6938 - val_loss: 0.6766 - val_categorical_accuracy: 0.7837\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.67099\n",
      "ROC integral: 0.7457789673246346\n",
      "ROC 1 = 0.7457789673246346\n",
      "best parameters: H:!V:VarTransform=N_AllClasses:FilenameModel=model_dense.h5:NumEpochs=10:BatchSize=100:TriesEarlyStopping=10\n",
      "best training: nTrain_Signal=80%:nTrain_Background=80%:SplitMode=Random:NormMode=NumEvents:!V\n",
      "best model: numero_HL=3, neurons=64, neurons_LF=2, k_init=glorot_normal, activation_IL=sigmoid, activation_HL=relu, activation_FL=softmax\n",
      "ROC integral: 0.7847150919911989\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.785\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.003 (0.000)       0.272 (0.275)      0.751 (0.655)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.746\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.006 (0.013)       0.314 (0.225)      0.693 (0.625)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n"
     ]
    }
   ],
   "source": [
    "param_opt(configs[2], params, training, model_input, comp_params, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
