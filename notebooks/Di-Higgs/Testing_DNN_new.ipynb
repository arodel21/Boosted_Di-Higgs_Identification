{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TMVA Classification Using Deep Neural Networks</center>\n",
    "\n",
    "In this notebook we still classify di-Higgs new data with Deep Neural Networks meethod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA, TTree\n",
    "import pandas as pd\n",
    "\n",
    "ROOT.TMVA.Tools.Instance()\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize()\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from array import array\n",
    "import numpy as np\n",
    "\n",
    "from root_numpy import root2array, tree2array\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by region.\n",
    "\n",
    "This function will let you filter your dataset by region. It's known that SR_1tag is very signal poor, while SR_2tag has a lot a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_region(file, region, signal):\n",
    "    oldfile = ROOT.TFile(file)\n",
    "    oldtree = oldfile.Nominal\n",
    "    signal_file = ROOT.TFile(region+\"_\"+signal+\"_s.root\",\"recreate\")\n",
    "    signal_tree = oldtree.CloneTree(0)\n",
    "    backg_file = ROOT.TFile(region+\"_\"+signal+\"_b.root\",\"recreate\")\n",
    "    backg_tree = oldtree.CloneTree(0)\n",
    "    data_file = ROOT.TFile(region+\"_\"+signal+\"_d.root\",\"recreate\")\n",
    "    data_tree = oldtree.CloneTree(0)\n",
    "    for entry in oldtree:\n",
    "        if (entry.m_region == region):\n",
    "            if (entry.sample == \"data\"):\n",
    "                data_tree.Fill()\n",
    "            elif (entry.sample == \"Xtohh1000\"): #signal\n",
    "                signal_tree.Fill()\n",
    "            else:\n",
    "                backg_tree.Fill()\n",
    "    signal_tree.AutoSave()   \n",
    "    backg_tree.AutoSave()\n",
    "    data_tree.AutoSave()\n",
    "    return signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file\n",
    "\n",
    "#Use as\n",
    "#tree, file = filter_region(\"data.root\", \"SR_1tag\", \"small.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory and Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.root has unlabeled data points (called data) and fakes points. For the background training we'll use only the fakes points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file = filter_region(\"all_1000.root\", \"SR_2tag\", \"Xtohh1000\")\n",
    "\n",
    "outputFile = ROOT.TFile.Open(\"Higgs_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "# Factory\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                      \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "#signal_tree.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input data abd variables \n",
    "\n",
    "We add first the signal and background trees in the data loader and then we\n",
    "define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "We have two kinds of signals and for the training we have to use only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree    ( signal_tree )\n",
    "loader.AddBackgroundTree( backg_tree )\n",
    "loader.SetWeightExpression(\"EventWeight\")\n",
    "\n",
    "not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "## Define input variables \n",
    "for branch in backg_tree.GetListOfBranches():\n",
    "    if branch.GetName() in not_cons:\n",
    "        continue\n",
    "    loader.AddVariable(branch.GetName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Setup the DataLoader by splitting events in training and test samples. \n",
    "Here we use a random split and a fixed number of training and test events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "loader.PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=70%:nTrain_Background=80%:SplitMode=Random:\"\n",
    "                                   \"NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               2200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 83,002\n",
      "Trainable params: 83,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, kernel_initializer='he_normal', activation='sigmoid', input_dim=10))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(200, kernel_initializer='he_normal', activation='softsign'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(200, kernel_initializer='he_normal', activation='softsign'))\n",
    "#model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(2, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['binary_accuracy'])\n",
    "\n",
    "# Store model to file\n",
    "model.save('model_dense.h5')\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ROOT.TMVA::MethodPyKeras object (\"Keras_Dense\") at 0x902e2e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, 'Keras_Dense',\n",
    "        'H:!V:VarTransform=N_AllClasses:FilenameModel=model_dense.h5:'+\\\n",
    "        'NumEpochs=200:BatchSize=32:TriesEarlyStopping=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/lib/python3.5/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 3466 samples\n",
      "Epoch 1/200\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.7360 - binary_accuracy: 0.5100 - val_loss: 8.8819e-04 - val_binary_accuracy: 0.4944\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00089, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6988 - binary_accuracy: 0.5167 - val_loss: 8.9081e-04 - val_binary_accuracy: 0.6353\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00089\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6725 - binary_accuracy: 0.5467 - val_loss: 8.6148e-04 - val_binary_accuracy: 0.2063\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00089 to 0.00086, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6964 - binary_accuracy: 0.5467 - val_loss: 8.8059e-04 - val_binary_accuracy: 0.9041\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00086\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6752 - binary_accuracy: 0.5400 - val_loss: 8.6625e-04 - val_binary_accuracy: 0.1375\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00086\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6444 - binary_accuracy: 0.5600 - val_loss: 8.3995e-04 - val_binary_accuracy: 0.4437\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00086 to 0.00084, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6115 - binary_accuracy: 0.6367 - val_loss: 8.7498e-04 - val_binary_accuracy: 0.8999\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00084\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6037 - binary_accuracy: 0.6500 - val_loss: 8.2039e-04 - val_binary_accuracy: 0.6317\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00084 to 0.00082, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5958 - binary_accuracy: 0.6733 - val_loss: 8.0527e-04 - val_binary_accuracy: 0.4964\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00082 to 0.00081, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5587 - binary_accuracy: 0.7067 - val_loss: 8.9147e-04 - val_binary_accuracy: 0.8989\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00081\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5734 - binary_accuracy: 0.6900 - val_loss: 7.8428e-04 - val_binary_accuracy: 0.5056\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00081 to 0.00078, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5137 - binary_accuracy: 0.7133 - val_loss: 8.4410e-04 - val_binary_accuracy: 0.8663\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00078\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5264 - binary_accuracy: 0.6900 - val_loss: 7.8224e-04 - val_binary_accuracy: 0.7343\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00078 to 0.00078, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4862 - binary_accuracy: 0.7067 - val_loss: 7.6683e-04 - val_binary_accuracy: 0.6704\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00078 to 0.00077, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4840 - binary_accuracy: 0.7167 - val_loss: 8.4628e-04 - val_binary_accuracy: 0.8681\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00077\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4866 - binary_accuracy: 0.7667 - val_loss: 7.6396e-04 - val_binary_accuracy: 0.5206\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00077 to 0.00076, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4955 - binary_accuracy: 0.6967 - val_loss: 8.7616e-04 - val_binary_accuracy: 0.8748\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00076\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6687 - binary_accuracy: 0.6200 - val_loss: 8.3429e-04 - val_binary_accuracy: 0.8523\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00076\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5342 - binary_accuracy: 0.6967 - val_loss: 8.4491e-04 - val_binary_accuracy: 0.3204\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00076\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4913 - binary_accuracy: 0.7000 - val_loss: 0.0011 - val_binary_accuracy: 0.9097\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00076\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6029 - binary_accuracy: 0.6467 - val_loss: 8.2732e-04 - val_binary_accuracy: 0.8446\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00076\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4538 - binary_accuracy: 0.7300 - val_loss: 7.6008e-04 - val_binary_accuracy: 0.6261\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00076 to 0.00076, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4620 - binary_accuracy: 0.7433 - val_loss: 8.0850e-04 - val_binary_accuracy: 0.8159\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00076\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4562 - binary_accuracy: 0.7500 - val_loss: 8.2170e-04 - val_binary_accuracy: 0.8319\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00076\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4566 - binary_accuracy: 0.7167 - val_loss: 7.6431e-04 - val_binary_accuracy: 0.7491\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00076\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4513 - binary_accuracy: 0.7467 - val_loss: 8.6333e-04 - val_binary_accuracy: 0.8710\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00076\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4126 - binary_accuracy: 0.7433 - val_loss: 7.5048e-04 - val_binary_accuracy: 0.6643\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00076 to 0.00075, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4094 - binary_accuracy: 0.7467 - val_loss: 8.9204e-04 - val_binary_accuracy: 0.8819\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00075\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4382 - binary_accuracy: 0.7333 - val_loss: 8.3686e-04 - val_binary_accuracy: 0.8501\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00075\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3767 - binary_accuracy: 0.7500 - val_loss: 7.5879e-04 - val_binary_accuracy: 0.7390\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00075\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3787 - binary_accuracy: 0.7300 - val_loss: 8.7783e-04 - val_binary_accuracy: 0.8781\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00075\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4632 - binary_accuracy: 0.7267 - val_loss: 7.4929e-04 - val_binary_accuracy: 0.7035\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00075 to 0.00075, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4176 - binary_accuracy: 0.7467 - val_loss: 8.2101e-04 - val_binary_accuracy: 0.8514\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00075\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4097 - binary_accuracy: 0.7433 - val_loss: 8.0055e-04 - val_binary_accuracy: 0.8467\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00075\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3891 - binary_accuracy: 0.7667 - val_loss: 8.4166e-04 - val_binary_accuracy: 0.8764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00075\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3851 - binary_accuracy: 0.7600 - val_loss: 7.3877e-04 - val_binary_accuracy: 0.7477\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00075 to 0.00074, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4003 - binary_accuracy: 0.7433 - val_loss: 9.0462e-04 - val_binary_accuracy: 0.9023\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00074\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4849 - binary_accuracy: 0.7267 - val_loss: 7.8844e-04 - val_binary_accuracy: 0.4830\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00074\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3767 - binary_accuracy: 0.7533 - val_loss: 0.0011 - val_binary_accuracy: 0.9188\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00074\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4337 - binary_accuracy: 0.7367 - val_loss: 7.6814e-04 - val_binary_accuracy: 0.8116\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00074\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3634 - binary_accuracy: 0.7533 - val_loss: 7.5614e-04 - val_binary_accuracy: 0.7999\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00074\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3703 - binary_accuracy: 0.7833 - val_loss: 9.0831e-04 - val_binary_accuracy: 0.9008\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00074\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3331 - binary_accuracy: 0.7933 - val_loss: 7.3557e-04 - val_binary_accuracy: 0.7180\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00074, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3977 - binary_accuracy: 0.7933 - val_loss: 8.2978e-04 - val_binary_accuracy: 0.8836\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00074\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3181 - binary_accuracy: 0.8033 - val_loss: 7.4831e-04 - val_binary_accuracy: 0.8032\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00074\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3369 - binary_accuracy: 0.8067 - val_loss: 7.8787e-04 - val_binary_accuracy: 0.8660\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00074\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3451 - binary_accuracy: 0.8000 - val_loss: 7.5979e-04 - val_binary_accuracy: 0.8439\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00074\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3386 - binary_accuracy: 0.7733 - val_loss: 9.6907e-04 - val_binary_accuracy: 0.9179\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00074\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3006 - binary_accuracy: 0.7633 - val_loss: 7.3151e-04 - val_binary_accuracy: 0.7422\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00074 to 0.00073, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3234 - binary_accuracy: 0.7967 - val_loss: 8.2552e-04 - val_binary_accuracy: 0.9033\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00073\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3259 - binary_accuracy: 0.8200 - val_loss: 7.6571e-04 - val_binary_accuracy: 0.8868\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00073\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3257 - binary_accuracy: 0.8400 - val_loss: 7.2668e-04 - val_binary_accuracy: 0.8292\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00073 to 0.00073, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3105 - binary_accuracy: 0.8133 - val_loss: 7.9026e-04 - val_binary_accuracy: 0.9022\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00073\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3277 - binary_accuracy: 0.8300 - val_loss: 7.5282e-04 - val_binary_accuracy: 0.8857\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00073\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3096 - binary_accuracy: 0.8000 - val_loss: 7.7804e-04 - val_binary_accuracy: 0.9038\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00073\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.2965 - binary_accuracy: 0.8300 - val_loss: 7.3147e-04 - val_binary_accuracy: 0.8677\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00073\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.2998 - binary_accuracy: 0.8133 - val_loss: 7.5054e-04 - val_binary_accuracy: 0.8911\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00073\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.2820 - binary_accuracy: 0.8133 - val_loss: 7.7545e-04 - val_binary_accuracy: 0.9008\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00073\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.2957 - binary_accuracy: 0.8333 - val_loss: 7.6106e-04 - val_binary_accuracy: 0.8853\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00073\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.2683 - binary_accuracy: 0.8300 - val_loss: 8.0895e-04 - val_binary_accuracy: 0.9153\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00073\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3134 - binary_accuracy: 0.8200 - val_loss: 7.7760e-04 - val_binary_accuracy: 0.6658\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00073\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3494 - binary_accuracy: 0.8167 - val_loss: 0.0010 - val_binary_accuracy: 0.9293\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00073\n",
      "Epoch 00062: early stopping\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.825\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.037 (0.184)       0.343 (0.782)      0.881 (0.986)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods()\n",
    "\n",
    "# Here we test all methods using the test data set\n",
    "factory.TestAllMethods()\n",
    "\n",
    "# Here we evaluate all methods and compare their performances, computing efficiencies, \n",
    "# ROC curves etc.. using both training and tetsing data sets. Several histograms are \n",
    "# produced which can be examined with the TMVAGui or directly using the output file\n",
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(file):\n",
    "    params = []\n",
    "    first = 1\n",
    "    with open(file, 'r') as fp:\n",
    "        line = fp.readline().rstrip()\n",
    "        while line:\n",
    "            if (file.split('.')[1] == 'csv' and first):\n",
    "                first = 0\n",
    "                line = fp.readline().rstrip()\n",
    "                continue\n",
    "            params.append(line)\n",
    "            line = fp.readline().rstrip()       \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(params, training, model_input, comp_params, model_name, config, comb):\n",
    "    \n",
    "    output_file = config+ \"_\" + comb + \"_DNN_Classification.root\"\n",
    "    signal_file = config+\"_s.root\"\n",
    "    backg_file = config+\"_b.root\"\n",
    "    \n",
    "    signal_input = ROOT.TFile(signal_file)\n",
    "    signal_tree = signal_input.Nominal\n",
    "    \n",
    "    backg_input = ROOT.TFile(backg_file)\n",
    "    backg_tree = backg_input.Nominal\n",
    "    \n",
    "    outputFile = ROOT.TFile.Open(output_file, \"RECREATE\")\n",
    "\n",
    "    # Factory\n",
    "    factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification_\"+config+\"_\"+comb, outputFile,\n",
    "                          \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "    loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "    ### global event weights per tree (see below for setting event-wise weights)\n",
    "    #signalWeight     = 1.0\n",
    "    #backgroundWeight = 1.0\n",
    "\n",
    "    ### You can add an arbitrary number of signal or background trees\n",
    "    loader.AddSignalTree    ( signal_tree )\n",
    "    loader.AddBackgroundTree( backg_tree )\n",
    "    loader.SetWeightExpression(\"EventWeight\")\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "    ## Define input variables \n",
    "    for branch in backg_tree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        loader.AddVariable(branch.GetName())\n",
    "        \n",
    "    mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "    mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "    loader.PrepareTrainingAndTestTree(mycuts, mycutb, training)\n",
    "    \n",
    "    # Model structure\n",
    "    \n",
    "    comp_params = comp_params.rstrip()\n",
    "    comp_params = comp_params.split(',')\n",
    "    loss = comp_params[0]\n",
    "    \n",
    "    comp_params.remove(loss)\n",
    "    metrics = comp_params\n",
    "    \n",
    "    model = Sequential()\n",
    "    model_input = model_input.rstrip()\n",
    "    model_input = model_input.split(',')\n",
    "    \n",
    "    hidden_l = int(model_input[0])\n",
    "    neurons = int(model_input[1])\n",
    "    neurons_LF = int(model_input[2])\n",
    "    k_init = model_input[3]\n",
    "    activation_IL = model_input[4]\n",
    "    activation_HL = model_input[5]\n",
    "    activation_FL = model_input[6]\n",
    "    \n",
    "    print(type(neurons))\n",
    "    \n",
    "    model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_IL, input_dim=10))\n",
    "    for h in range(hidden_l):\n",
    "        model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_HL))\n",
    "        \n",
    "    model.add(Dense(neurons_LF, kernel_initializer=k_init, activation=activation_FL))\n",
    "    \n",
    "    # Set loss and optimizer\n",
    "    model.compile(loss=loss, optimizer=Adam(), metrics=metrics)\n",
    "    # Store model to file\n",
    "    model.save(model_name)\n",
    "    # Print summary of model\n",
    "    model.summary()\n",
    "    \n",
    "    ## DNN method\n",
    "    factory.BookMethod(loader,ROOT.TMVA.Types.kPyKeras, \"Keras_Dense\", params)\n",
    "        \n",
    "    factory.TrainAllMethods()\n",
    "    \n",
    "    factory.TestAllMethods()\n",
    "    \n",
    "    factory.EvaluateAllMethods()\n",
    "    \n",
    "    c1 = factory.GetROCCurve(loader)\n",
    "    #c1.Draw()\n",
    "    \n",
    "    integ = factory.GetROCIntegral(loader, \"Keras_Dense\")\n",
    "    \n",
    "    print(\"ROC integral:\", integ)\n",
    "    \n",
    "    outputFile.Close()\n",
    "    signal_input.Close()\n",
    "    backg_input.Close()\n",
    "    \n",
    "    return integ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_combs_params(file_params, file_training, file_model, comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics):\n",
    "    comb_params = list(itertools.product(arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics))\n",
    "    with open(file_params, 'w') as params, open(file_training, 'w') as training, open(file_model, 'w') as model, open(comp_params, 'w') as comp_p:\n",
    "        model.write(\"number_HL,neurons,neurons_LF,k_init,activation_IL,activation_HL,activation_FL\\n\")\n",
    "        for cp in comb_params:\n",
    "            string1 = \"H:!V:VarTransform=N_AllClasses:FilenameModel=\"+model_name+\":NumEpochs=\"+str(cp[0])+\":BatchSize=\"+str(cp[1])+\":TriesEarlyStopping=10\\n\"\n",
    "            params.write(string1)\n",
    "            string2 = \"nTrain_Signal=\"+str(cp[2])+\"%:nTrain_Background=\"+str(cp[3])+\"%:SplitMode=Random:NormMode=NumEvents:!V\\n\"\n",
    "            training.write(string2)\n",
    "            string3 = str(cp[4])+','+str(cp[5])+','+str(cp[6])+','+str(cp[7])+','+str(cp[8])+','+str(cp[9])+','+str(cp[10])+'\\n'\n",
    "            model.write(string3)\n",
    "            string4 = str(cp[11])+','+str(cp[12])+'\\n'\n",
    "            comp_p.write(string4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_params=\"dnn_params2.txt\"\n",
    "file_training=\"dnn_training2.txt\"\n",
    "file_model=\"dnn_model2.csv\"\n",
    "file_comp_params='comp_params.txt'\n",
    "model_name=\"model_dense.h5\"\n",
    "arr_NumEpochs=[100]\n",
    "arr_BatchSize=[64]\n",
    "arr_nTrain_Signal=[80]\n",
    "arr_nTrain_Background=[80]\n",
    "arr_number_HL=[3]\n",
    "arr_neurons=[100]\n",
    "arr_neurons_LF=[2]\n",
    "arr_k_init=['he_uniform']\n",
    "arr_activation_IL=['relu']\n",
    "arr_activation_HL=['selu']\n",
    "arr_activation_FL=['softmax']\n",
    "arr_loss=['poisson']\n",
    "arr_metrics=[f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_combs_params(file_params, file_training, file_model, file_comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_opt(config, params, training, model, comp_params, model_name):\n",
    "    max_roc = 0\n",
    "    best_params = \"\"\n",
    "    best_train = \"\"\n",
    "    best_model = \"\"\n",
    "    print(config)\n",
    "    print(\"===============\")\n",
    "    for i in range(len(params)):\n",
    "        roc = DNN(params[i], training[i], model[i], comp_params[i], model_name, config)\n",
    "        print(\"i:\",i)\n",
    "        print(\"ROC:\", roc)\n",
    "        if roc > max_roc:\n",
    "            max_roc = roc\n",
    "            best_params = params[i]\n",
    "            best_train = training[i]\n",
    "            best_model = model[i]\n",
    "            best_comp = comp_params[i]\n",
    "    best_model = best_model.split(',')\n",
    "    best_model_str = \"numero_HL=\"+str(best_model[0])+\", neurons=\"+str(best_model[1])+\", neurons_LF=\"+str(best_model[2])+\", k_init=\"+str(best_model[3])+\", activation_IL=\"+str(best_model[4])+\", activation_HL=\"+str(best_model[5])+\", activation_FL=\"+str(best_model[6])\n",
    "    best_comp = best_comp.split(',')\n",
    "    best_comp_str = \"loss=\"+best_comp[0]+\", metrics=\"+best_comp[1]\n",
    "    print(\"best parameters:\", best_params)\n",
    "    print(\"best training:\", best_train)\n",
    "    print(\"best model:\", best_model_str)\n",
    "    print(\"best comps:\", best_comp_str)\n",
    "    print(\"ROC integral:\", max_roc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"dnn_params2.txt\"\n",
    "params = get_params(params_path)\n",
    "training_path = \"dnn_training2.txt\"\n",
    "training = get_params(training_path)\n",
    "configs = [\"PreSel_0tag_Xtohh1000\", \"PreSel_1tag_Xtohh1000\", \"PreSel_2tag_Xtohh1000\", \n",
    "           \"QCDCR_0tag_Xtohh1000\", \"QCDCR_1tag_Xtohh1000\", \"QCDCR_2tag_Xtohh1000\",\n",
    "           \"SR_0tag_Xtohh1000\", \"SR_1tag_Xtohh1000\", \"SR_2tag_Xtohh1000\",\n",
    "           \"PreSel_0tag_Xtohh2000\", \"PreSel_1tag_Xtohh2000\", \"PreSel_2tag_Xtohh2000\",\n",
    "           \"QCDCR_0tag_Xtohh2000\", \"QCDCR_1tag_Xtohh2000\", \"QCDCR_2tag_Xtohh2000\",\n",
    "           \"SR_0tag_Xtohh2000\", \"SR_1tag_Xtohh2000\", \"SR_2tag_Xtohh2000\"]\n",
    "s_end = \"_s.root\"\n",
    "b_end = \"_b.root\"\n",
    "comp_params_path = \"comp_params.txt\"\n",
    "comp_params = get_params(comp_params_path)\n",
    "model_input_path = \"dnn_model2.csv\"\n",
    "model_input = get_params(model_input_path)\n",
    "model_name = \"model_dense.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_ = \"H:!V:VarTransform=N_AllClasses:FilenameModel=model_dense.h5:NumEpochs=300:BatchSize=64:TriesEarlyStopping=10\"\n",
    "#model_input_ = \"3,20,2,he_uniform,relu,selu,softmax\"\n",
    "#comp_params_ = \"binary_crossentropy,binary_accuracy\"\n",
    "#training_ = \"nTrain_Signal=80%:nTrain_Background=70%:SplitMode=Random:NormMode=NumEvents:!V\"\n",
    "#model_name_ = \"model_dense.h5\"\n",
    "#comb = \"COMB5\"\n",
    "#DNN(params_, training_, model_input_, comp_params_, model_name_, configs[17], comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_opt(configs[17], params, training, model_input, comp_params, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weights\n",
    "def v_out(tree, not_cons, variables, reader):\n",
    "    \n",
    "    h = ROOT.TH1D(\"\",\"\",60,-1,1)\n",
    "    \n",
    "    nevt = tree.GetEntries()\n",
    "\n",
    "    vout = np.arange(nevt, dtype='float').reshape(1, nevt)\n",
    "\n",
    "    for ievt, entry in enumerate(tree):\n",
    "        i = 0    \n",
    "        for branch in tree.GetListOfBranches():\n",
    "            name = branch.GetName()\n",
    "            if name in not_cons:\n",
    "                continue\n",
    "            variables[i][0] = getattr(entry,name)\n",
    "            i += 1\n",
    "\n",
    "        vout[0,ievt] = reader.EvaluateMVA(methodName)\n",
    "        h.Fill(vout[0,ievt])\n",
    "    \n",
    "    return h, vout, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_report_pred(background, signal, data, sep):\n",
    "    background = list(background[0])\n",
    "    signal = list(signal[0])\n",
    "    data = list(data[0])\n",
    "    bakg_t = [0]*len(background)\n",
    "    signal_t = [1]*len(signal)\n",
    "    y_predicted = background + signal\n",
    "    y_test = bakg_t + signal_t\n",
    "    for i in range(len(y_predicted)):\n",
    "        if (y_predicted[i] < sep):\n",
    "            y_predicted[i] = 0\n",
    "        else:\n",
    "            y_predicted[i] = 1\n",
    "    for j in range(len(data)):\n",
    "        if (data[j] < sep):\n",
    "            data[j] = 0\n",
    "        else:\n",
    "            data[j] = 1\n",
    "    print(classification_report(y_test, y_predicted, target_names=[\"background\", \"signal\"]))\n",
    "    print(\"Accuracy total:\", accuracy_score(y_test, y_predicted))\n",
    "    #print(\"Confusion matrix:\", confusion_matrix(y_test, data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config, type_signal, methodName, sep, comb):\n",
    "    reader = TMVA.Reader( \"!Color:!Silent\" )\n",
    "    \n",
    "    dataPath = config + \"_\" + type_signal + \"_d.root\"\n",
    "    bkgPath = config + \"_\" + type_signal + \"_b.root\"\n",
    "    sigPath = config + \"_\" + type_signal + \"_s.root\"\n",
    "    \n",
    "    print(dataPath)\n",
    "    \n",
    "    dataFile = ROOT.TFile(dataPath)\n",
    "    bkgFile = ROOT.TFile(bkgPath)\n",
    "    sigFile = ROOT.TFile(sigPath)\n",
    "\n",
    "    dataTree = dataFile.Nominal\n",
    "    bkgTree = bkgFile.Nominal\n",
    "    sigTree = sigFile.Nominal\n",
    "    \n",
    "    # Add Variables: We add variables to the reader exactly in the same way we did for the **DataLoader** during the training\n",
    "    # We need to specify the address of the variable in order to pass it to TMVA when we iterate on the TTree\n",
    "    variables = []\n",
    "    i = 0\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "    \n",
    "    for branch in dataTree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        aux = array('f',[0])\n",
    "        variables.append(aux)\n",
    "        reader.AddVariable(branch.GetName(),variables[i])\n",
    "        i = i+1\n",
    "    \n",
    "    # Setup Classifiers: We set up the classifiers by reading the input weights from the appropriate files\n",
    "    # The file is stored for example as *dataset/weights/TMVAClassification_BDT.weights.xml\n",
    "    weightfile = \"dataset/weights/TMVA_Higgs_Classification_\"+ config + \"_\" + type_signal + \"_\" + comb + \"_\" + methodName + \".weights.xml\" #_\" + config + \"_\" + type_signal + \"_\" + comb + \"_\" + methodName + \"\n",
    "    name = ROOT.TString(methodName)\n",
    "    reader.BookMVA( name, weightfile )\n",
    "    \n",
    "    # We iterate on the input event in the given TTree. We provide as input first the background tree \n",
    "    # We return the output results for the various methods in big numpy array [ number of methods x \n",
    "    # number of events]\n",
    "    # We also fill an histogram for each method.\n",
    "    # Note that is important to fill the arrays with the tree entries in order to pass the values to \n",
    "    # the TMVA::Reader\n",
    "    variables2 = variables\n",
    "    \n",
    "    hd, d_vout, variables = v_out(dataTree, not_cons, variables, reader)\n",
    "    hs, s_vout, variables = v_out(sigTree, not_cons, variables, reader)\n",
    "    hb, b_vout, variables = v_out(bkgTree, not_cons, variables, reader)\n",
    "    \n",
    "    print(\"Signal size:\", len(s_vout[0]))\n",
    "    print(\"Background size:\", len(b_vout[0]))\n",
    "    \n",
    "    # Classification report\n",
    "    prediction = gen_report_pred(b_vout, s_vout, d_vout, sep)\n",
    "    \n",
    "    print(\"Prediction size:\", len(prediction))\n",
    "    \n",
    "    # Histogram\n",
    "    c1 = ROOT.TCanvas(\"c1\", \"c1\")\n",
    "    c1.cd()\n",
    "    hb.Draw()\n",
    "    hs.SetLineColor(ROOT.kRed)\n",
    "    hd.SetFillColor(ROOT.kGreen)\n",
    "    hb.SetFillColor(ROOT.kBlue)\n",
    "    hb.Draw()\n",
    "    hs.Draw('Same')\n",
    "    hd.Draw('Same')\n",
    "    hb.SetTitle(\"background\")\n",
    "    hs.SetTitle(\"signal\")\n",
    "    hd.SetTitle(\"data\")\n",
    "    c1.BuildLegend()\n",
    "        \n",
    "    img_file = config + \"_\" + type_signal + \"_\" + comb + \"_\" + methodName + \"_hist.png\"\n",
    "    c1.SaveAs(img_file)\n",
    "    \n",
    "    dataFile.Close()\n",
    "    sigFile.Close()\n",
    "    bkgFile.Close()\n",
    "    \n",
    "    display(Image(filename=img_file)) \n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def predict_test(config, type_signal, methodName, sep, comb):\n",
    "    \n",
    "    reader = TMVA.Reader( \"!Color:!Silent\" )\n",
    "    data_input = config + \"_\" + type_signal + \"_\" + comb + \"_DNN_Classification.root\"\n",
    "    data = ROOT.TFile(data_input)\n",
    "    testTree = data.Get(\"dataset/TestTree\")\n",
    "    \n",
    "    array_ = tree2array(testTree)\n",
    "    className = pd.DataFrame(array_)\n",
    "    className = className['className']\n",
    "    true = []\n",
    "    \n",
    "    for el in className:\n",
    "        if el == b'Background':\n",
    "            true.append(0)\n",
    "        else:\n",
    "            true.append(1)\n",
    "    \n",
    "    variables = []\n",
    "    i = 0\n",
    "    \n",
    "    not_cons = ['classID', 'className', 'weight', 'Keras_Dense']\n",
    "    \n",
    "    for branch in testTree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        aux = array('f',[0])\n",
    "        variables.append(aux)\n",
    "        reader.AddVariable(branch.GetName(),variables[i])\n",
    "        i = i+1\n",
    "    \n",
    "    weightfile = \"dataset/weights/TMVA_Higgs_Classification_\"+ config + \"_\" + type_signal + \"_\" + comb + \"_\" + methodName + \".weights.xml\" #_\" + config + \"_\" + type_signal + \"_\" + comb + \"_\" + methodName + \"\n",
    "    name = ROOT.TString(methodName)\n",
    "    reader.BookMVA( name, weightfile )\n",
    "    \n",
    "    ht, t_vout, variables = v_out(testTree, not_cons, variables, reader)\n",
    "    \n",
    "    pred = []\n",
    "    for el in t_vout[0]:\n",
    "        if el <= sep:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    \n",
    "    print(\"Test set size:\", len(t_vout[0]))\n",
    "\n",
    "    print(classification_report(true, pred))\n",
    "    print(\"Accuracy:\", accuracy_score(true, pred))\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(matrix[0])\n",
    "    print(matrix[1]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 11728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.01      0.38      0.03       359\n",
      "          1       0.86      0.12      0.22     11369\n",
      "\n",
      "avg / total       0.84      0.13      0.21     11728\n",
      "\n",
      "Accuracy: 0.13071282401091405\n",
      "Confusion matrix:\n",
      "[135 224]\n",
      "[9971 1398]\n",
      "                         : Booking \"Keras_Dense\" of type \"PyKeras\" from dataset/weights/TMVA_Higgs_Classification_SR_2tag_Xtohh2000_COMB2_Keras_Dense.weights.xml.\n",
      "                         : Reading weight file: dataset/weights/TMVA_Higgs_Classification_SR_2tag_Xtohh2000_COMB2_Keras_Dense.weights.xml\n",
      "<HEADER> DataSetInfo              : [Default] : Added class \"Signal\"\n",
      "<HEADER> DataSetInfo              : [Default] : Added class \"Background\"\n",
      "                         : Booked classifier \"Keras_Dense\" of type: \"PyKeras\"\n",
      "                         : Load model from file: dataset/weights/TrainedModel_Keras_Dense.h5\n"
     ]
    }
   ],
   "source": [
    "config = \"SR_2tag\"\n",
    "type_signal = \"Xtohh2000\"\n",
    "methodName = \"Keras_Dense\"\n",
    "sep = 0.5\n",
    "comb = \"COMB2\"\n",
    "asd = predict_test(config, type_signal, methodName, sep, comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
