{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TMVA Classification Using Deep Neural Networks</center>\n",
    "\n",
    "In this notebook we still classify di-Higgs new data with Deep Neural Networks meethod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA, TTree\n",
    "import pandas as pd\n",
    "\n",
    "ROOT.TMVA.Tools.Instance()\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize()\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from array import array\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by region.\n",
    "\n",
    "This function will let you filter your dataset by region. It's known that SR_1tag is very signal poor, while SR_2tag has a lot a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_region(file, region, signal):\n",
    "    oldfile = ROOT.TFile(file)\n",
    "    oldtree = oldfile.Nominal\n",
    "    signal_file = ROOT.TFile(region+\"_\"+signal+\"_s.root\",\"recreate\")\n",
    "    signal_tree = oldtree.CloneTree(0)\n",
    "    backg_file = ROOT.TFile(region+\"_\"+signal+\"_b.root\",\"recreate\")\n",
    "    backg_tree = oldtree.CloneTree(0)\n",
    "    data_file = ROOT.TFile(region+\"_\"+signal+\"_d.root\",\"recreate\")\n",
    "    data_tree = oldtree.CloneTree(0)\n",
    "    for entry in oldtree:\n",
    "        if (entry.m_region == region):\n",
    "            if (entry.sample == \"data\"):\n",
    "                data_tree.Fill()\n",
    "            elif (entry.sample == \"Xtohh1000\"): #signal\n",
    "                signal_tree.Fill()\n",
    "            else:\n",
    "                backg_tree.Fill()\n",
    "    signal_tree.AutoSave()   \n",
    "    backg_tree.AutoSave()\n",
    "    data_tree.AutoSave()\n",
    "    return signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file\n",
    "\n",
    "#Use as\n",
    "#tree, file = filter_region(\"data.root\", \"SR_1tag\", \"small.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory and Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.root has unlabeled data points (called data) and fakes points. For the background training we'll use only the fakes points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tree, signal_file, backg_tree, backg_file, data_tree, data_file = filter_region(\"all_1000.root\", \"SR_1tag\", \"Xtohh1000\")\n",
    "\n",
    "outputFile = ROOT.TFile.Open(\"Higgs_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "# Factory\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                      \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "#signal_tree.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input data abd variables \n",
    "\n",
    "We add first the signal and background trees in the data loader and then we\n",
    "define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "We have two kinds of signals and for the training we have to use only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree    ( signal_tree )\n",
    "loader.AddBackgroundTree( backg_tree )\n",
    "loader.SetSignalWeightExpression(\"EventWeight\")\n",
    "\n",
    "not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "## Define input variables \n",
    "for branch in backg_tree.GetListOfBranches():\n",
    "    if branch.GetName() in not_cons:\n",
    "        continue\n",
    "    loader.AddVariable(branch.GetName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Setup the DataLoader by splitting events in training and test samples. \n",
    "Here we use a random split and a fixed number of training and test events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "loader.PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=70%:nTrain_Background=70%:SplitMode=Random:\"\n",
    "                                   \"NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-01 16:42:27.151399: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2195135000 Hz\n",
      "2019-10-01 16:42:27.152008: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8b20510 executing computations on platform Host. Devices:\n",
      "2019-10-01 16:42:27.152028: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-10-01 16:42:27.195688: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='sigmoid', input_dim=10))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy',])\n",
    "\n",
    "# Store model to file\n",
    "model.save('model_dense.h5')\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ROOT.TMVA::MethodPyKeras object (\"Keras_Dense\") at 0x89a63e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, 'Keras_Dense',\n",
    "        'H:!V:VarTransform=G:FilenameModel=model_dense.h5:'+\\\n",
    "        'NumEpochs=10:BatchSize=16:TriesEarlyStopping=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.6880 - categorical_accuracy: 0.5089 - val_loss: 0.6264 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62642, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 0s 166us/step - loss: 0.6611 - categorical_accuracy: 0.6786 - val_loss: 0.6032 - val_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62642 to 0.60316, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 0s 166us/step - loss: 0.6358 - categorical_accuracy: 0.7500 - val_loss: 0.5779 - val_categorical_accuracy: 0.8571\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60316 to 0.57790, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.6188 - categorical_accuracy: 0.7232 - val_loss: 0.5470 - val_categorical_accuracy: 0.8214\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.57790 to 0.54698, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.5931 - categorical_accuracy: 0.7143 - val_loss: 0.5208 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54698 to 0.52080, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 0s 142us/step - loss: 0.5732 - categorical_accuracy: 0.7411 - val_loss: 0.5015 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52080 to 0.50147, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 0s 132us/step - loss: 0.5679 - categorical_accuracy: 0.7143 - val_loss: 0.4793 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50147 to 0.47934, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 0s 175us/step - loss: 0.5525 - categorical_accuracy: 0.7411 - val_loss: 0.4670 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47934 to 0.46697, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 0s 129us/step - loss: 0.5571 - categorical_accuracy: 0.7500 - val_loss: 0.4635 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.46697 to 0.46350, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 0s 169us/step - loss: 0.5395 - categorical_accuracy: 0.7411 - val_loss: 0.4769 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.46350\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.829\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.021 (0.072)       0.468 (0.620)      0.828 (0.872)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods()\n",
    "\n",
    "# Here we test all methods using the test data set\n",
    "factory.TestAllMethods()\n",
    "\n",
    "# Here we evaluate all methods and compare their performances, computing efficiencies, \n",
    "# ROC curves etc.. using both training and tetsing data sets. Several histograms are \n",
    "# produced which can be examined with the TMVAGui or directly using the output file\n",
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(file):\n",
    "    params = []\n",
    "    first = 1\n",
    "    with open(file, 'r') as fp:\n",
    "        line = fp.readline().rstrip()\n",
    "        while line:\n",
    "            if (file.split('.')[1] == 'csv' and first):\n",
    "                first = 0\n",
    "                line = fp.readline().rstrip()\n",
    "                continue\n",
    "            params.append(line)\n",
    "            line = fp.readline().rstrip()       \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(params, training, model_input, comp_params, model_name, config):\n",
    "    \n",
    "    output_file = config+\"_DNN_Classification.root\"\n",
    "    signal_file = config+\"_s.root\"\n",
    "    backg_file = config+\"_b.root\"\n",
    "    \n",
    "    signal_input = ROOT.TFile(signal_file)\n",
    "    signal_tree = signal_input.Nominal\n",
    "    \n",
    "    backg_input = ROOT.TFile(backg_file)\n",
    "    backg_tree = backg_input.Nominal\n",
    "    \n",
    "    outputFile = ROOT.TFile.Open(output_file, \"RECREATE\")\n",
    "\n",
    "    # Factory\n",
    "    factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification_\"+config, outputFile,\n",
    "                          \"!V:ROC:Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )\n",
    "\n",
    "    loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "    ### global event weights per tree (see below for setting event-wise weights)\n",
    "    #signalWeight     = 1.0\n",
    "    #backgroundWeight = 1.0\n",
    "\n",
    "    ### You can add an arbitrary number of signal or background trees\n",
    "    loader.AddSignalTree    ( signal_tree )\n",
    "    loader.AddBackgroundTree( backg_tree )\n",
    "    loader.SetSignalWeightExpression(\"EventWeight\")\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "\n",
    "    ## Define input variables \n",
    "    for branch in backg_tree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        loader.AddVariable(branch.GetName())\n",
    "        \n",
    "    mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "    mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "    loader.PrepareTrainingAndTestTree(mycuts, mycutb, training)\n",
    "    \n",
    "    # Model structure\n",
    "    \n",
    "    comp_params = comp_params.rstrip()\n",
    "    comp_params = comp_params.split(',')\n",
    "    loss = comp_params[0]\n",
    "    \n",
    "    comp_params.remove(loss)\n",
    "    metrics = comp_params\n",
    "    \n",
    "    model = Sequential()\n",
    "    model_input = model_input.rstrip()\n",
    "    model_input = model_input.split(',')\n",
    "    \n",
    "    hidden_l = int(model_input[0])\n",
    "    neurons = int(model_input[1])\n",
    "    neurons_LF = int(model_input[2])\n",
    "    k_init = model_input[3]\n",
    "    activation_IL = model_input[4]\n",
    "    activation_HL = model_input[5]\n",
    "    activation_FL = model_input[6]\n",
    "    \n",
    "    print(type(neurons))\n",
    "    \n",
    "    model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_IL, input_dim=10))\n",
    "    for h in range(hidden_l):\n",
    "        model.add(Dense(neurons, kernel_initializer=k_init, activation=activation_HL))\n",
    "        \n",
    "    model.add(Dense(neurons_LF, kernel_initializer=k_init, activation=activation_FL))\n",
    "    \n",
    "    # Set loss and optimizer\n",
    "    model.compile(loss=loss, optimizer=Adam(), metrics=metrics)\n",
    "    # Store model to file\n",
    "    model.save(model_name)\n",
    "    # Print summary of model\n",
    "    model.summary()\n",
    "    \n",
    "    ## DNN method\n",
    "    factory.BookMethod(loader,ROOT.TMVA.Types.kPyKeras, \"Keras_Dense\", params)\n",
    "        \n",
    "    factory.TrainAllMethods()\n",
    "    \n",
    "    factory.TestAllMethods()\n",
    "    \n",
    "    factory.EvaluateAllMethods()\n",
    "    \n",
    "    c1 = factory.GetROCCurve(loader)\n",
    "    #c1.Draw()\n",
    "    \n",
    "    integ = factory.GetROCIntegral(loader, \"Keras_Dense\")\n",
    "    \n",
    "    print(\"ROC integral:\", integ)\n",
    "    \n",
    "    outputFile.Close()\n",
    "    signal_input.Close()\n",
    "    backg_input.Close()\n",
    "    \n",
    "    return integ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_combs_params(file_params, file_training, file_model, comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics):\n",
    "    comb_params = list(itertools.product(arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics))\n",
    "    with open(file_params, 'w') as params, open(file_training, 'w') as training, open(file_model, 'w') as model, open(comp_params, 'w') as comp_p:\n",
    "        model.write(\"number_HL,neurons,neurons_LF,k_init,activation_IL,activation_HL,activation_FL\\n\")\n",
    "        for cp in comb_params:\n",
    "            string1 = \"H:!V:VarTransform=N_AllClasses:FilenameModel=\"+model_name+\":NumEpochs=\"+str(cp[0])+\":BatchSize=\"+str(cp[1])+\":TriesEarlyStopping=10\\n\"\n",
    "            params.write(string1)\n",
    "            string2 = \"nTrain_Signal=\"+str(cp[2])+\"%:nTrain_Background=\"+str(cp[3])+\"%:SplitMode=Random:NormMode=NumEvents:!V\\n\"\n",
    "            training.write(string2)\n",
    "            string3 = str(cp[4])+','+str(cp[5])+','+str(cp[6])+','+str(cp[7])+','+str(cp[8])+','+str(cp[9])+','+str(cp[10])+'\\n'\n",
    "            model.write(string3)\n",
    "            string4 = str(cp[11])+','+str(cp[12])+'\\n'\n",
    "            comp_p.write(string4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_params=\"dnn_params2.txt\"\n",
    "file_training=\"dnn_training2.txt\"\n",
    "file_model=\"dnn_model2.csv\"\n",
    "file_comp_params='comp_params.txt'\n",
    "model_name=\"model_dense.h5\"\n",
    "arr_NumEpochs=[10]\n",
    "arr_BatchSize=[100, 200]\n",
    "arr_nTrain_Signal=[80]\n",
    "arr_nTrain_Background=[80]\n",
    "arr_number_HL=[3]\n",
    "arr_neurons=[64]\n",
    "arr_neurons_LF=[2]\n",
    "arr_k_init=['glorot_normal']\n",
    "arr_activation_IL=['sigmoid']\n",
    "arr_activation_HL=['relu']\n",
    "arr_activation_FL=['softmax']\n",
    "arr_loss=['categorical_crossentropy']\n",
    "arr_metrics=['categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_combs_params(file_params, file_training, file_model, file_comp_params, model_name, arr_NumEpochs, arr_BatchSize, arr_nTrain_Signal, arr_nTrain_Background, arr_number_HL, arr_neurons, arr_neurons_LF, arr_k_init, arr_activation_IL, arr_activation_HL, arr_activation_FL, arr_loss, arr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_opt(config, params, training, model, comp_params, model_name):\n",
    "    max_roc = 0\n",
    "    best_params = \"\"\n",
    "    best_train = \"\"\n",
    "    best_model = \"\"\n",
    "    print(config)\n",
    "    print(\"===============\")\n",
    "    for i in range(len(params)):\n",
    "        roc = DNN(params[i], training[i], model[i], comp_params[i], model_name, config)\n",
    "        if roc > max_roc:\n",
    "            max_roc = roc\n",
    "            best_params = params[i]\n",
    "            best_train = training[i]\n",
    "            best_model = model[i]\n",
    "    best_model = best_model.split(',')\n",
    "    best_model_str = \"numero_HL=\"+str(best_model[0])+\", neurons=\"+str(best_model[1])+\", neurons_LF=\"+str(best_model[2])+\", k_init=\"+str(best_model[3])+\", activation_IL=\"+str(best_model[4])+\", activation_HL=\"+str(best_model[5])+\", activation_FL=\"+str(best_model[6])\n",
    "    print(\"best parameters:\", best_params)\n",
    "    print(\"best training:\", best_train)\n",
    "    print(\"best model:\", best_model_str)\n",
    "    print(\"ROC integral:\", max_roc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"dnn_params2.txt\"\n",
    "params = get_params(params_path)\n",
    "training_path = \"dnn_training2.txt\"\n",
    "training = get_params(training_path)\n",
    "configs = [\"PreSel_0tag_Xtohh1000\", \"PreSel_1tag_Xtohh1000\", \"PreSel_2tag_Xtohh1000\", \n",
    "           \"QCDCR_0tag_Xtohh1000\", \"QCDCR_1tag_Xtohh1000\", \"QCDCR_2tag_Xtohh1000\",\n",
    "           \"SR_0tag_Xtohh1000\", \"SR_1tag_Xtohh1000\", \"SR_2tag_Xtohh1000\",\n",
    "           \"PreSel_0tag_Xtohh2000\", \"PreSel_1tag_Xtohh2000\", \"PreSel_2tag_Xtohh2000\",\n",
    "           \"QCDCR_0tag_Xtohh2000\", \"QCDCR_1tag_Xtohh2000\", \"QCDCR_2tag_Xtohh2000\",\n",
    "           \"SR_0tag_Xtohh2000\", \"SR_1tag_Xtohh2000\", \"SR_2tag_Xtohh2000\"]\n",
    "s_end = \"_s.root\"\n",
    "b_end = \"_b.root\"\n",
    "comp_params_path = \"comp_params.txt\"\n",
    "comp_params = get_params(comp_params_path)\n",
    "model_input_path = \"dnn_model2.csv\"\n",
    "model_input = get_params(model_input_path)\n",
    "model_name = \"model_dense.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/10\n",
      "128/128 [==============================] - 0s 3ms/step - loss: 0.7077 - categorical_accuracy: 0.4375 - val_loss: 0.6700 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66999, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.7006 - categorical_accuracy: 0.4922 - val_loss: 0.6763 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66999\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 0s 94us/step - loss: 0.7009 - categorical_accuracy: 0.5078 - val_loss: 0.6774 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.66999\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 0s 118us/step - loss: 0.7012 - categorical_accuracy: 0.5078 - val_loss: 0.6726 - val_categorical_accuracy: 0.4688\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.66999\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 0s 101us/step - loss: 0.7007 - categorical_accuracy: 0.5000 - val_loss: 0.6667 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66999 to 0.66670, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 0s 75us/step - loss: 0.6985 - categorical_accuracy: 0.5391 - val_loss: 0.6650 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.66670 to 0.66496, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 0s 77us/step - loss: 0.6976 - categorical_accuracy: 0.5547 - val_loss: 0.6647 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.66496 to 0.66470, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 0s 74us/step - loss: 0.6962 - categorical_accuracy: 0.5859 - val_loss: 0.6643 - val_categorical_accuracy: 0.5625\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.66470 to 0.66428, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 0s 83us/step - loss: 0.6955 - categorical_accuracy: 0.5625 - val_loss: 0.6645 - val_categorical_accuracy: 0.5312\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66428\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 0s 99us/step - loss: 0.6951 - categorical_accuracy: 0.5469 - val_loss: 0.6656 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66428\n",
      "ROC integral: 0.7091585847900198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7091585847900198"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.709\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.021 (0.023)       0.337 (0.325)      0.640 (0.602)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 8294\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 480267\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n"
     ]
    }
   ],
   "source": [
    "DNN(params[0], training[0], model_input[0], comp_params[0], model_name, configs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreSel_2tag_Xtohh1000\n",
      "===============\n",
      "<class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 160 samples, validate on 5178 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6917 - categorical_accuracy: 0.5188 - val_loss: 0.1218 - val_categorical_accuracy: 0.6288\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12177, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 668us/step - loss: 0.6876 - categorical_accuracy: 0.6437 - val_loss: 0.1237 - val_categorical_accuracy: 0.8384\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12177\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 631us/step - loss: 0.6864 - categorical_accuracy: 0.6125 - val_loss: 0.1220 - val_categorical_accuracy: 0.8067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12177\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 613us/step - loss: 0.6833 - categorical_accuracy: 0.6312 - val_loss: 0.1129 - val_categorical_accuracy: 0.1904\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12177 to 0.11294, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 624us/step - loss: 0.6816 - categorical_accuracy: 0.5313 - val_loss: 0.1147 - val_categorical_accuracy: 0.2480\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11294\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 621us/step - loss: 0.6785 - categorical_accuracy: 0.5750 - val_loss: 0.1194 - val_categorical_accuracy: 0.7086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11294\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 656us/step - loss: 0.6757 - categorical_accuracy: 0.6938 - val_loss: 0.1240 - val_categorical_accuracy: 0.8789\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11294\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 640us/step - loss: 0.6746 - categorical_accuracy: 0.6250 - val_loss: 0.1279 - val_categorical_accuracy: 0.8621\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11294\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 675us/step - loss: 0.6705 - categorical_accuracy: 0.6313 - val_loss: 0.1186 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11294\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 571us/step - loss: 0.6668 - categorical_accuracy: 0.7000 - val_loss: 0.1117 - val_categorical_accuracy: 0.4112\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11294 to 0.11169, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "ROC integral: 0.8203347113320818\n",
      "<class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,314\n",
      "Trainable params: 13,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 160 samples, validate on 5178 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6915 - categorical_accuracy: 0.6062 - val_loss: 0.1190 - val_categorical_accuracy: 0.1796\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11904, saving model to dataset/weights/TrainedModel_Keras_Dense.h5\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 414us/step - loss: 0.6892 - categorical_accuracy: 0.4938 - val_loss: 0.1197 - val_categorical_accuracy: 0.2223\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.11904\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 370us/step - loss: 0.6882 - categorical_accuracy: 0.5437 - val_loss: 0.1225 - val_categorical_accuracy: 0.7082\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11904\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 332us/step - loss: 0.6871 - categorical_accuracy: 0.7250 - val_loss: 0.1243 - val_categorical_accuracy: 0.8694\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11904\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 384us/step - loss: 0.6861 - categorical_accuracy: 0.6750 - val_loss: 0.1245 - val_categorical_accuracy: 0.8694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11904\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 348us/step - loss: 0.6850 - categorical_accuracy: 0.6687 - val_loss: 0.1233 - val_categorical_accuracy: 0.8598\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11904\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 363us/step - loss: 0.6839 - categorical_accuracy: 0.7188 - val_loss: 0.1218 - val_categorical_accuracy: 0.6698\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11904\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 390us/step - loss: 0.6827 - categorical_accuracy: 0.7250 - val_loss: 0.1205 - val_categorical_accuracy: 0.4861\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11904\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 386us/step - loss: 0.6814 - categorical_accuracy: 0.6313 - val_loss: 0.1202 - val_categorical_accuracy: 0.4820\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11904\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 339us/step - loss: 0.6799 - categorical_accuracy: 0.6313 - val_loss: 0.1204 - val_categorical_accuracy: 0.5527\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11904\n",
      "ROC integral: 0.76485230262862\n",
      "best parameters: H:!V:VarTransform=N_AllClasses:FilenameModel=model_dense.h5:NumEpochs=10:BatchSize=100:TriesEarlyStopping=10\n",
      "best training: nTrain_Signal=80%:nTrain_Background=80%:SplitMode=Random:NormMode=NumEvents:!V\n",
      "best model: numero_HL=3, neurons=64, neurons_LF=2, k_init=glorot_normal, activation_IL=sigmoid, activation_HL=relu, activation_FL=softmax\n",
      "ROC integral: 0.8203347113320818\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.820\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.003 (0.033)       0.373 (0.245)      0.806 (0.675)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       Keras_Dense    : 0.765\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              Keras_Dense    : 0.001 (0.000)       0.239 (0.250)      0.672 (0.775)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: ROCCurve dataset class 0\n"
     ]
    }
   ],
   "source": [
    "param_opt(configs[2], params, training, model_input, comp_params, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weights\n",
    "def v_out(tree, not_cons, variables, methodName, reader, hname):\n",
    "    \n",
    "    h = {}\n",
    "    h[methodName] = (ROOT.TH1D(hname,\"htitle_\"+hname,60,-1,1))\n",
    "    \n",
    "    nevt = tree.GetEntries()\n",
    "\n",
    "    vout = np.arange(nevt, dtype='float').reshape(1, nevt)\n",
    "\n",
    "    for ievt, entry in enumerate(tree):\n",
    "        i = 0    \n",
    "        for branch in tree.GetListOfBranches():\n",
    "            name = branch.GetName()\n",
    "            if name in not_cons:\n",
    "                continue\n",
    "            variables[i][0] = getattr(entry,name)\n",
    "            i += 1\n",
    "\n",
    "        vout[0,ievt] = reader.EvaluateMVA(methodName)\n",
    "        h[methodName].Fill(vout[0,ievt])\n",
    "    \n",
    "    return h, vout, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_report_pred(background, signal, data, sep):\n",
    "    background = list(background[0])\n",
    "    signal = list(signal[0])\n",
    "    data = list(data[0])\n",
    "    bakg_t = [0]*len(background)\n",
    "    signal_t = [1]*len(signal)\n",
    "    y_predicted = background + signal\n",
    "    y_test = bakg_t + signal_t\n",
    "    for i in range(len(y_predicted)):\n",
    "        if (y_predicted[i] < sep):\n",
    "            y_predicted[i] = 0\n",
    "        else:\n",
    "            y_predicted[i] = 1\n",
    "    for j in range(len(data)):\n",
    "        if (data[j] < sep):\n",
    "            data[j] = 0\n",
    "        else:\n",
    "            data[j] = 1\n",
    "    print(classification_report(y_test, y_predicted, target_names=[\"background\", \"signal\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config, type_signal, methodName, sep):\n",
    "    reader = TMVA.Reader( \"!Color:!Silent\" )\n",
    "    \n",
    "    dataPath = config + \"_\" + type_signal + \"_d.root\"\n",
    "    bkgPath = config + \"_\" + type_signal + \"_b.root\"\n",
    "    sigPath = config + \"_\" + type_signal + \"_s.root\"\n",
    "    \n",
    "    print(dataPath)\n",
    "    \n",
    "    dataFile = ROOT.TFile(dataPath)\n",
    "    bkgFile = ROOT.TFile(bkgPath)\n",
    "    sigFile = ROOT.TFile(sigPath)\n",
    "\n",
    "    dataTree = dataFile.Nominal\n",
    "    bkgTree = bkgFile.Nominal\n",
    "    sigTree = sigFile.Nominal\n",
    "    \n",
    "    # Add Variables: We add variables to the reader exactly in the same way we did for the **DataLoader** during the training\n",
    "    # We need to specify the address of the variable in order to pass it to TMVA when we iterate on the TTree\n",
    "    variables = []\n",
    "    i = 0\n",
    "    \n",
    "    not_cons = ['sample', 'EventWeight', 'EventNumber', 'm_region', 'm_FJNbtagJets', 'm_FJphi', 'm_FJeta', 'm_DTeta', 'm_DTphi']\n",
    "    \n",
    "    for branch in dataTree.GetListOfBranches():\n",
    "        if branch.GetName() in not_cons:\n",
    "            continue\n",
    "        aux = array('f',[0])\n",
    "        variables.append(aux)\n",
    "        reader.AddVariable(branch.GetName(),variables[i])\n",
    "        i = i+1\n",
    "    \n",
    "    # Setup Classifiers: We set up the classifiers by reading the input weights from the appropriate files\n",
    "    # The file is stored for example as *dataset/weights/TMVAClassification_BDT.weights.xml\n",
    "    weightfile = \"dataset/weights/TMVA_Higgs_Classification_\" + config + \"_\" + type_signal + \"_\" + methodName + \".weights.xml\"\n",
    "    name = ROOT.TString(methodName)\n",
    "    reader.BookMVA( name, weightfile )\n",
    "    \n",
    "    # We iterate on the input event in the given TTree. We provide as input first the background tree \n",
    "    # We return the output results for the various methods in big numpy array [ number of methods x \n",
    "    # number of events]\n",
    "    # We also fill an histogram for each method.\n",
    "    # Note that is important to fill the arrays with the tree entries in order to pass the values to \n",
    "    # the TMVA::Reader\n",
    "    hd, d_vout, variables = v_out(dataTree, not_cons, variables, methodName, reader, \"data\")\n",
    "    hs, s_vout, variables = v_out(sigTree, not_cons, variables, methodName, reader, \"signal\")\n",
    "    hb, b_vout, variables = v_out(bkgTree, not_cons, variables, methodName, reader, \"bkg\")\n",
    "    \n",
    "    # Classification report\n",
    "    prediction = gen_report_pred(b_vout, s_vout, d_vout, sep)\n",
    "    \n",
    "    # Histogram\n",
    "    '''c1 = ROOT.TCanvas()\n",
    "    pad = c1.cd(0)\n",
    "    h1 = hb[methodName]\n",
    "    h1.Draw()\n",
    "    h2 = hs[methodName]\n",
    "    h2.SetLineColor(ROOT.kRed)\n",
    "    h3 = hd[methodName]\n",
    "    h3.SetFillColor(ROOT.kGreen)\n",
    "    h1.SetFillColor(ROOT.kBlue)\n",
    "    h2.Draw('Same')\n",
    "    h3.Draw('Same')\n",
    "    pad.BuildLegend()\n",
    "    c1.Modified()\n",
    "    c1.Update()\n",
    "\n",
    "    c1.Draw()\n",
    "    c1.Print()'''\n",
    "    \n",
    "    dataFile.Close()\n",
    "    sigFile.Close()\n",
    "    bkgFile.Close()\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_1tag_Xtohh1000_d.root\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.86      0.15      0.26      9262\n",
      "      signal       0.30      0.94      0.45      3553\n",
      "\n",
      "    accuracy                           0.37     12815\n",
      "   macro avg       0.58      0.54      0.35     12815\n",
      "weighted avg       0.71      0.37      0.31     12815\n",
      "\n",
      "                         : Booking \"Keras_Dense\" of type \"PyKeras\" from dataset/weights/TMVA_Higgs_Classification_SR_1tag_Xtohh1000_Keras_Dense.weights.xml.\n",
      "                         : Reading weight file: dataset/weights/TMVA_Higgs_Classification_SR_1tag_Xtohh1000_Keras_Dense.weights.xml\n",
      "<WARNING> <WARNING>                : Value for option tensorboard was previously set to \n",
      "<HEADER> DataSetInfo              : [Default] : Added class \"Signal\"\n",
      "<HEADER> DataSetInfo              : [Default] : Added class \"Background\"\n",
      "                         : Booked classifier \"Keras_Dense\" of type: \"PyKeras\"\n",
      "                         : Load model from file: dataset/weights/TrainedModel_Keras_Dense.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_d.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_d.root, recovered key TTree:Nominal at address 244\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_b.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_b.root, recovered key TTree:Nominal at address 480267\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n",
      "Warning in <TFile::Init>: file SR_1tag_Xtohh1000_s.root probably not closed, trying to recover\n",
      "Info in <TFile::Recover>: SR_1tag_Xtohh1000_s.root, recovered key TTree:Nominal at address 8294\n",
      "Warning in <TFile::Init>: successfully recovered 1 keys\n"
     ]
    }
   ],
   "source": [
    "config = \"SR_1tag\"\n",
    "type_signal = \"Xtohh1000\"\n",
    "methodName = \"Keras_Dense\"\n",
    "sep = 0.5\n",
    "asd = predict(config, type_signal, methodName, sep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
